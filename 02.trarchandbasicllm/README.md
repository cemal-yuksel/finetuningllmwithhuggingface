```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                                â•‘
â•‘        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â•‘
â•‘        â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â•‘
â•‘          â–ˆâ–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â•‘
â•‘         â–ˆâ–ˆâ•”â•â•â• â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â•šâ•â•â•â•â–ˆâ–ˆâ•‘â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â•‘
â•‘        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘   â•‘
â•‘        â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â• â•šâ•â•â•â•â•â•  â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•   â•‘
â•‘                                                                                â•‘
â•‘                 ğŸ§  Transformer Architecture & Attention Mechanism             â•‘
â•‘                  ğŸ“ Modern AI Models'un Kalbinde Yatan SÄ±rlar                â•‘
â•‘                                                                                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

# ğŸš€ Transformer Mimarisi ve Dikkat MekanizmasÄ± - EÄŸitim ModÃ¼lleri

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![Jupyter](https://img.shields.io/badge/Jupyter-Notebook-orange.svg)](https://jupyter.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Status](https://img.shields.io/badge/Status-Active-brightgreen.svg)]()

---

## ğŸ“– Ä°Ã§erik HaritasÄ±

- [Proje HakkÄ±nda](#proje-hakkÄ±nda)
- [Dosya YapÄ±sÄ±](#dosya-yapÄ±sÄ±)
- [EÄŸitim ModÃ¼lleri](#eÄŸitim-modÃ¼lleri)
- [Ã–ÄŸrenme Ã‡Ä±ktÄ±larÄ±](#Ã¶ÄŸrenme-Ã§Ä±ktÄ±larÄ±)
- [Teknoloji Stack](#teknoloji-stack)
- [Kurulum ve BaÅŸlangÄ±Ã§](#kurulum-ve-baÅŸlangÄ±Ã§)
- [Derin Ä°Ã§elik Ã–zeti](#derin-iÃ§elik-Ã¶zeti)
- [Kaynaklar ve Referanslar](#kaynaklar-ve-referanslar)

---

### ğŸ¯ HÄ±zlÄ± Ã–zet - Transformer Mimarisi

```mermaid
flowchart LR
    INPUT["ğŸ“ INPUT<br/>Ali bankadan<br/>kredi Ã§ekti"] --> EMB["ğŸ”„ Embedding<br/>Positional<br/>Encoding"]
    
    EMB --> ENC["ğŸ”µ ENCODER"]
    ENC --> ENCA["ğŸ”— Self-Attention<br/>â†“<br/>ğŸ§  Feed-Forward<br/>â†“<br/>â†» N katman"]
    ENCA --> CTX["ğŸ“Š Context<br/>Vectors"]
    
    CTX --> DEC["ğŸŸ¢ DECODER"]
    DEC --> DECA["ğŸ”— Self-Attention<br/>â†“<br/>ğŸ”„ Cross-Attention<br/>â†“<br/>â†» N katman"]
    DECA --> OUT["âœ… OUTPUT<br/>Ã‡ay zamanÄ±<br/>geldi"]
    
    CTX -.-> DECA
    
    style INPUT fill:#E8D7F1,stroke:#B59FCA,stroke-width:3px,color:#333
    style EMB fill:#D4EDDA,stroke:#7CB342,stroke-width:2px,color:#333
    style ENC fill:#BBDEFB,stroke:#1976D2,stroke-width:3px,color:#333
    style ENCA fill:#C5E1A5,stroke:#558B2F,stroke-width:2px,color:#333
    style CTX fill:#B2EBF2,stroke:#0097A7,stroke-width:3px,color:#333
    style DEC fill:#C8E6C9,stroke:#388E3C,stroke-width:3px,color:#333
    style DECA fill:#F0F4C3,stroke:#9E9D24,stroke-width:2px,color:#333
    style OUT fill:#C8E6C9,stroke:#2E7D32,stroke-width:3px,color:#333
```

---

### ğŸ§  Dikkat MekanizmasÄ± - Temel AkÄ±ÅŸ

```mermaid
graph LR
    A["ğŸ“ Ã§ekti"] --> B["ğŸ” Q: Query"]
    A --> C["ğŸ”‘ K: Keys"]
    A --> D["ğŸ’ V: Values"]
    
    B --> E["ğŸ“Š QÂ·K Benzerlik"]
    C --> E
    
    E --> F1["Ali: 0.25"]
    E --> F2["bankadan: 0.85â­"]
    E --> F3["kredi: 0.90â­"]
    E --> F4["Ã§ekti: 0.10"]
    
    F1 & F2 & F3 & F4 --> G["âš–ï¸ Softmax"]
    
    G --> H1["Ali: 10%"]
    G --> H2["bankadan: 30%"]
    G --> H3["kredi: 45%"]
    G --> H4["Ã§ekti: 15%"]
    
    H1 & H2 & H3 & H4 --> I["âœ… AÄŸÄ±rlÄ±klÄ± Toplam"]
    D --> I
    I --> J["ğŸ¯ Kredi Ã‡ekme Ä°ÅŸlemi"]
    
    style A fill:#E8D7F1,stroke:#B59FCA,stroke-width:2px
    style B fill:#C5E1A5,stroke:#558B2F,stroke-width:2px
    style C fill:#BBDEFB,stroke:#1976D2,stroke-width:2px
    style D fill:#FFE0B2,stroke:#E65100,stroke-width:2px
    style E fill:#D1C4E9,stroke:#512DA8,stroke-width:2px
    style G fill:#B2EBF2,stroke:#0097A7,stroke-width:2px
    style I fill:#F8BBD0,stroke:#C2185B,stroke-width:2px
    style J fill:#C8E6C9,stroke:#2E7D32,stroke-width:3px
```

---

## ğŸ“š Proje HakkÄ±nda

Bu proje, **Hugging Face Transformers** ile Fine-Tuning Ã¶ÄŸreniminin ikinci modÃ¼lÃ¼dÃ¼r. Modern yapay zeka modellerinin (ChatGPT, BERT, GPT-4 vb.) temelinde yatan **Transformer mimarisi** ve Ã¶zellikle **Dikkat MekanizmasÄ± (Attention Mechanism)** konularÄ±nÄ± **derinlemesine ve uygulamalÄ±** olarak Ã¶ÄŸretmeyi amaÃ§lar.

### ğŸ¯ AmaÃ§

- âœ… Seq2Seq (Sequence-to-Sequence) modellerini anlamak
- âœ… Encoder-Decoder mimarisinin nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± Ã¶ÄŸrenmek
- âœ… Dikkat mekanizmasÄ±nÄ±n temel kavramlarÄ±nÄ± (Q, K, V) aÃ§Ä±klamak
- âœ… Transformer yapÄ±sÄ±nÄ±n matematiksel temelini kurmak
- âœ… Pratik Python uygulamalarÄ±yla bilgiyi pekiÅŸtirmek

### ğŸŒŸ Hedef Kitle

- ğŸ“ Bilgisayar MÃ¼hendisliÄŸi Ã¶ÄŸrencileri
- ğŸ”¬ YBS Ã¶ÄŸrencileri
- ğŸ’¼ Makine Ã–ÄŸrenmesi meraklÄ±larÄ±
- ğŸš€ NLP (DoÄŸal Dil Ä°ÅŸleme) pratisyenleri

---

## ğŸ“ Dosya YapÄ±sÄ±

```mermaid
graph TD
    ROOT["ğŸ—‚ï¸ 02.trarchandbasicllm"]
    
    ROOT --> NB1["ğŸ““ 01.seq2seqmodels.ipynb"]
    ROOT --> NB2["ğŸ““ 02.qkv-dec-enc.ipynb"]
    ROOT --> PDF["ğŸ“„ Transformers PDF"]
    ROOT --> README["ğŸ“– README.md"]
    
    NB1 --> NB1A["ğŸ¯ Seq2Seq KavramlarÄ±"]
    NB1 --> NB1B["ğŸ”§ Encoder-Decoder"]
    NB1 --> NB1C["âš ï¸ Sorunlar & Ã‡Ã¶zÃ¼m"]
    
    NB2 --> NB2A["ğŸ¬ BÃ¶lÃ¼m 1-2<br/>Q,K,V & Analiz"]
    NB2 --> NB2B["ğŸ¬ BÃ¶lÃ¼m 3-4<br/>Ã–lÃ§ekleme & Softmax"]
    NB2 --> NB2C["ğŸ¬ BÃ¶lÃ¼m 5-6<br/>Maskeleme & Multi-Head"]
    NB2 --> NB2D["ğŸ¬ BÃ¶lÃ¼m 7-8<br/>Cross-Attention"]
    NB2 --> NB2E["ğŸ¬ BÃ¶lÃ¼m 9-10<br/>Python & Quiz"]
    
    style ROOT fill:#FFE0B2,stroke:#E65100,stroke-width:3px
    style NB1 fill:#C5E1A5,stroke:#558B2F,stroke-width:2px
    style NB2 fill:#BBDEFB,stroke:#1976D2,stroke-width:2px
    style PDF fill:#F8BBD0,stroke:#C2185B,stroke-width:2px
    style README fill:#B2EBF2,stroke:#0097A7,stroke-width:2px
```

---

## ğŸ“ EÄŸitim ModÃ¼lleri

### ğŸ““ 1. Seq2Seq Modelleri ve Attention MekanizmasÄ±
**Dosya:** `01.seq2seqmodels.ipynb` (1800 satÄ±r)

#### ğŸ“Œ Ä°Ã§eriÄŸi:
- **Seq2Seq Nedir?** - GiriÅŸ ve temel fikir
  - Makine Ã§evirisi, metin Ã¶zetleme, soru cevaplama Ã¶rnekleri
  - GÃ¼nlÃ¼k hayattan benzetmeler (dinle â†’ anla â†’ anlat)
  
- **Encoder-Decoder Mimarisi** - DetaylÄ± aÃ§Ä±klama
  - ğŸ”µ ENCODER (KodlayÄ±cÄ±): Kelimeleri sÄ±rayla iÅŸler, gizli durumlar oluÅŸturur
  - ğŸŸ¢ DECODER (Kod Ã‡Ã¶zÃ¼cÃ¼): Ã–zet vektÃ¶rden yola Ã§Ä±karak yeni cÃ¼mle Ã¼retir
  - Otoregresif Ã¼retim (Autoregressive Generation)
  
- **BÃ¼yÃ¼k Sorunlar: Neden Yeterli DeÄŸil?**
  - âš ï¸ Bilgi DarboÄŸazÄ± (Information Bottleneck)
  - âš ï¸ SÄ±ralÄ± Ä°ÅŸlem (Sequential Processing)
  - ğŸ“Š Performans grafiÄŸi: CÃ¼mle uzunluÄŸu vs baÅŸarÄ± iliÅŸkisi
  
- **Ã‡Ã¶zÃ¼m Fikri: Dikkat MekanizmasÄ±**
  - TÃ¼m gizli durumlarÄ± saklama
  - GerektiÄŸinde dÃ¶nÃ¼p bakmak (Attention)

#### ğŸ¯ Ã–ÄŸrenme Hedefleri:
- [ ] Seq2Seq modellerin temel Ã§alÄ±ÅŸma prensibini anlama
- [ ] Encoder ve Decoder'Ä±n rollerini ayÄ±rt edebilme
- [ ] Klasik Seq2Seq'nin sÄ±nÄ±rlamalarÄ±nÄ± fark etme
- [ ] Dikkat mekanizmasÄ±nÄ±n neden gerekli olduÄŸunu aÃ§Ä±klayabilme

---

### ğŸ¯ 2. Dikkat MekanizmasÄ±: Q, K, V ve TransformatÃ¶r Mimarisi
**Dosya:** `02.qkv-dec-enc.ipynb` (5265 satÄ±r)

#### ğŸ“Œ BÃ¶lÃ¼mler:

**ğŸ¬ BÃ–LÃœM 1: YouTube Analojisi ile Q, K, V KavramlarÄ±**
- YouTube arama sistemi paralelliÄŸi
- ğŸ” Query (Q): AradÄ±ÄŸÄ±nÄ±z terim
- ğŸ”‘ Key (K): Video baÅŸlÄ±klarÄ±, etiketleri
- ğŸ’ Value (V): Videonun gerÃ§ek iÃ§eriÄŸi
- Benzerlik puanlarÄ± ve aÄŸÄ±rlÄ±klandÄ±rma

**ğŸ¬ BÃ–LÃœM 2: CÃ¼mle Analizi - Dikkat Pratikte**
- GerÃ§ek cÃ¼mle Ã¶rneÄŸi: "Ali bankadan kredi Ã§ekti"
- "Ã§ekti" kelimesinin belirsizliÄŸi
- BaÄŸlamsal anlam nasÄ±l Ã§Ä±karÄ±lÄ±r?
- Dikkat matrisinin gÃ¶rselleÅŸtirilmesi

**ğŸ¬ BÃ–LÃœM 3: Ã–lÃ§ekli Nokta Ã‡arpÄ±m (Scaled Dot-Product)**
- Nokta Ã§arpÄ±m: Q Â· K
- Neden âˆšd_k ile bÃ¶lÃ¼yoruz?
- Ã–lÃ§eklemenin matematiksel nedenleri
- BÃ¼yÃ¼k boyutlarda stabilite

**ğŸ¬ BÃ–LÃœM 4: Softmax - AÄŸÄ±rlÄ±k DaÄŸÄ±lÄ±mÄ±**
- Softmax fonksiyonu nedir?
- FormÃ¼l: Ïƒ(x_i) = exp(x_i) / Î£ exp(x_j)
- SÄ±caklÄ±k (Temperature) parametresi
- OlasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±na dÃ¶nÃ¼ÅŸÃ¼m

**ğŸ¬ BÃ–LÃœM 5: Maskeleme - GeleceÄŸi GÃ¶rmeme KuralÄ±**
- Decoder maskeleme neden gerekli?
- Otoregresif Ã¼retimde bilgi sÄ±zÄ±ntÄ±sÄ±nÄ± Ã¶nlemek
- Causal maskeleme uygulamasÄ±

**ğŸ¬ BÃ–LÃœM 6: Ã‡ok KafalÄ± Dikkat (Multi-Head Attention)**
- Neden sadece tek dikkat yetmez?
- Paralel dikkat baÅŸlarÄ±
- Her baÅŸÄ±n farklÄ± perspektifi
- SonuÃ§larÄ±n birleÅŸtirilmesi

**ğŸ¬ BÃ–LÃœM 7: Encoder-Decoder Mimarisi**
- Transformer Ã§evirmen mimarisi
- Cross-Attention mekanizmasÄ±
- Encoder Ã§Ä±ktÄ±sÄ±nÄ± Decoder'a geÃ§mek

**ğŸ¬ BÃ–LÃœM 8: Konumsal Kodlama (Positional Encoding)**
- Kelimelerin sÄ±rasÄ±nÄ±n Ã¶nemi
- SinÃ¼s/kosinÃ¼s tabanlÄ± konumsal vektÃ¶rler
- Transformer'a sÄ±ra bilgisini ekleme

**ğŸ¬ BÃ–LÃœM 9: Python UygulamasÄ±**
- Numpy ile attention mekanizmasÄ± kodlamasÄ±
- AdÄ±m adÄ±m hesaplama
- GÃ¶rselleÅŸtirme ve analiz
- Basit bir dikkat katmanÄ± yazma

**ğŸ¬ BÃ–LÃœM 10: Ã–zet ve Quiz**
- TÃ¼m kavramlarÄ±n pekiÅŸtirilmesi
- Kendini test etme sorularÄ±
- SÄ±nÄ±rlamalar ve ileri konular

#### ğŸ¯ Ã–ÄŸrenme Hedefleri:
- [ ] Q, K, V kavramlarÄ±nÄ± YouTube Ã¶rneÄŸi ile anlama
- [ ] Ã–lÃ§ekli nokta Ã§arpÄ±mÄ± hesaplayabilme
- [ ] Softmax ve maskelemeyi aÃ§Ä±klayabilme
- [ ] Dikkat matrislerini yorumlayabilme
- [ ] Ã‡ok kafalÄ± dikkat mimarisini anlaÅŸabilme
- [ ] Python'da attention mekanizmasÄ± kodlayabilme

---

## ğŸ¯ Ã–ÄŸrenme Ã‡Ä±ktÄ±larÄ±

Bu modÃ¼lleri tamamladÄ±ÄŸÄ±nÄ±zda aÅŸaÄŸÄ±dakileri yapabilir olacaksÄ±nÄ±z:

### ğŸ”§ Teknik Beceriler
- âœ… Transformer mimarisinin temel yapÄ±sÄ±nÄ± anlama
- âœ… Dikkat mekanizmasÄ±nÄ±n her adÄ±mÄ±nÄ± matematiksel olarak hesaplama
- âœ… FarklÄ± dikkat stratejilerini (self-attention, cross-attention) tanÄ±ma
- âœ… NumPy ile dikkat katmanlarÄ± yazma

### ğŸ§  Kavramsal AnlayÄ±ÅŸ
- âœ… Neden Seq2Seq modelleri yetersiz olduÄŸunu aÃ§Ä±klama
- âœ… Dikkat mekanizmasÄ±nÄ±n "bilgi bottleneck" sorununu Ã§Ã¶zmeyi anlama
- âœ… Q, K, V'nin rolÃ¼nÃ¼ baÄŸlamsal olarak aÃ§Ä±klama
- âœ… Ã–lÃ§ekleme ve softmax'in neden gerekli olduÄŸunu anlama

### ğŸ“Š Pratik Uygulamalar
- âœ… Dikkat aÄŸÄ±rlÄ±klarÄ±nÄ± gÃ¶rselleÅŸtirme
- âœ… Dikkat matrislerini analiz etme
- âœ… FarklÄ± cÃ¼mlelerde modeli test etme
- âœ… Hata ayÄ±klama ve model davranÄ±ÅŸÄ±nÄ± anlama

---

## âš™ï¸ Teknoloji Stack

### ï¿½ Technology Stack - Visual Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TECHNOLOGY ECOSYSTEM                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                   CORE ENVIRONMENT                        â•‘
    â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
    â•‘  ğŸ Python 3.8+   â† Temel programlama dili              â•‘
    â•‘  ğŸ““ Jupyter      â† Ä°nteraktif notebook ortamÄ±           â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘              SCIENTIFIC COMPUTING LAYER                   â•‘
    â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
    â•‘  NumPy        â†’ SayÄ±sal hesaplamalar                     â•‘
    â•‘  Matplotlib   â†’ Grafikler ve diagramlar                 â•‘
    â•‘  Seaborn      â†’ Ä°leri gÃ¶rselleÅŸtirme                    â•‘
    â•‘  SciPy        â†’ Ä°statistik ve optimizasyon              â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘           DEEP LEARNING FRAMEWORKS (Ä°steÄŸe baÄŸlÄ±)         â•‘
    â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
    â•‘  PyTorch            â†’ Tensor iÅŸlemleri                   â•‘
    â•‘  Hugging Face ğŸ¤—   â†’ Transformers kÃ¼tÃ¼phanesi          â•‘
    â•‘  TensorFlow         â†’ Alternatif framework              â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                OUTPUT & DEPLOYMENT                        â•‘
    â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
    â•‘  IPython/Jupyter  â†’ EtkileÅŸimli Ã§alÄ±ÅŸma                 â•‘
    â•‘  Git              â†’ Versiyon kontrolÃ¼                    â•‘
    â•‘  VS Code          â†’ Kod editÃ¶rÃ¼                          â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

###ï¿½ğŸ“š KÃ¼tÃ¼phaneler
- **NumPy** - SayÄ±sal hesaplama
- **Matplotlib** - GÃ¶rselleÅŸtirme
- **Seaborn** - GeliÅŸmiÅŸ gÃ¶rselleÅŸtirme (Ä±sÄ± haritalarÄ±)
- **IPython** - Jupyter interaktifliÄŸi
- **Transformers (Hugging Face)** - GerÃ§ek model uygulamalarÄ±

### ğŸ’» Sistem Gereksinimleri
- **Python:** 3.8+
- **RAM:** 4GB minimum (8GB Ã¶nerilir)
- **GPU:** Ä°steÄŸe baÄŸlÄ± (CPU ile Ã§alÄ±ÅŸÄ±r)
- **OS:** Windows, Mac, Linux

---

## ğŸš€ Kurulum ve BaÅŸlangÄ±Ã§

### 1ï¸âƒ£ Gerekli Paketleri YÃ¼kleyin

```bash
# Temel bilimsel kÃ¼tÃ¼phaneler
pip install numpy matplotlib seaborn

# Jupyter Notebook
pip install jupyter

# (Ä°steÄŸe baÄŸlÄ±) Hugging Face Transformers
pip install transformers torch
```

### 2ï¸âƒ£ Notebook'u AÃ§Ä±n

```bash
# Proje klasÃ¶rÃ¼ne gidin
cd path/to/02.trarchandbasicllm

# Jupyter'i baÅŸlatÄ±n
jupyter notebook

# TarayÄ±cÄ±da ÅŸu dosyalarÄ± aÃ§Ä±n:
# - 01.seq2seqmodels.ipynb
# - 02.qkv-dec-enc.ipynb
```

### 3ï¸âƒ£ NasÄ±l Bir SÄ±ra Ä°zlemeliyim?

```mermaid
graph LR
    S(["ğŸš€ BaÅŸla"]) --> S1["ğŸ““ Seq2Seq<br/>Notebook"]
    S1 --> S2["ğŸ§  KavramlarÄ±<br/>PekiÅŸtir"]
    S2 --> S3["ğŸ¯ Dikkat<br/>Notebook"]
    S3 --> S4["ğŸ¥ YouTube<br/>Analojisi"]
    S4 --> S5["ğŸ“ Matematik<br/>Hesaplamalar"]
    S5 --> S6["ğŸ’» Python<br/>KodlarÄ±"]
    S6 --> S7["ğŸ“ Quiz<br/>Testi"]
    S7 --> E(["ğŸ‰ BaÅŸarÄ±lÄ±!"])
    
    style S fill:#C8E6C9,stroke:#2E7D32,stroke-width:3px
    style S1 fill:#C5E1A5,stroke:#558B2F,stroke-width:2px
    style S2 fill:#BBDEFB,stroke:#1976D2,stroke-width:2px
    style S3 fill:#FFE0B2,stroke:#E65100,stroke-width:2px
    style S4 fill:#F8BBD0,stroke:#C2185B,stroke-width:2px
    style S5 fill:#B2EBF2,stroke:#0097A7,stroke-width:2px
    style S6 fill:#D1C4E9,stroke:#512DA8,stroke-width:2px
    style S7 fill:#E0BBE4,stroke:#7B1FA2,stroke-width:2px
    style E fill:#C8E6C9,stroke:#2E7D32,stroke-width:3px
```

---

## ğŸ” Derin Ä°Ã§erik Ã–zeti

### ğŸ“– Temel Kavramlar GlossarÄ±

| Kavram | AÃ§Ä±klama | Ã–rnek |
|--------|----------|-------|
| **Sequence** | SÄ±ralÄ± veri (kelime dizileri) | "BugÃ¼n hava gÃ¼zel" |
| **Embedding** | Kelimeyi sayÄ±sal vektÃ¶re Ã§evirme | [0.5, -0.3, 0.8, ...] |
| **Hidden State** | Encoder'Ä±n ara hesaplamalarÄ±nda oluÅŸan vektÃ¶r | hâ‚, hâ‚‚, hâ‚ƒ |
| **Context Vector** | Encoder'Ä±n son Ã§Ä±ktÄ±sÄ± (Ã¶zet) | Tek bir vektÃ¶r |
| **Query (Q)** | AnlamlandÄ±rÄ±lacak kelime | "Ã§ekti" |
| **Key (K)** | KarÅŸÄ±laÅŸtÄ±rma iÃ§in Ã¶zellikler | TÃ¼m kelimelerin temsilleri |
| **Value (V)** | GerÃ§ek bilgi paketleri | Kelimelerin anlamlarÄ± |
| **Attention Score** | Q ve K benzerliÄŸi | 0.95, 0.15, 0.88 |
| **Softmax** | PuanlarÄ± aÄŸÄ±rlÄ±klara Ã§evirme | [0.45, 0.40, 0.10, 0.05] |
| **Attention Weight** | Normalize edilmiÅŸ dikkat aÄŸÄ±rlÄ±ÄŸÄ± | %45, %40, %10, %5 |
| **Scaled Dot-Product** | QÂ·K / âˆšd_k formÃ¼lÃ¼ | Stabilize edilmiÅŸ puan |
| **Multi-Head** | Paralel dikkat baÅŸlarÄ± | 8 veya 12 dikkat baÅŸÄ± |

### ğŸ§© Temel FormÃ¼ller

#### Ã–lÃ§ekli Nokta Ã‡arpÄ±m Dikkat

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

Nedir bu formÃ¼l?
- Q: Query vektÃ¶rleri (anlamlandÄ±rÄ±lacak)
- K: Key vektÃ¶rleri (karÅŸÄ±laÅŸtÄ±rmak iÃ§in)
- V: Value vektÃ¶rleri (Ã§Ä±kÄ±ÅŸ bilgisi)
- d_k: Key vektÃ¶rÃ¼nÃ¼n boyutu
- âˆšd_k: Ã–lÃ§ekleme faktÃ¶rÃ¼

#### Softmax

$$\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}}$$

- Herhangi bir sayÄ±yÄ± 0-1 arasÄ±na Ã§evirir
- TÃ¼m Ã§Ä±ktÄ±larÄ±n toplamÄ± tam olarak 1.0'dÄ±r
- OlasÄ±lÄ±k daÄŸÄ±lÄ±mÄ± oluÅŸturur

### ğŸ¬ Kritik Noktalar

#### âš ï¸ Problem #1: Bilgi DarboÄŸazÄ±
```
Uzun cÃ¼mle (20+ kelime) 
    â†“
TÃ¼mÃ¼ tek vektÃ¶re sÄ±kÄ±ÅŸtÄ±rÄ±lÄ±r 
    â†“
BaÅŸlangÄ±Ã§taki bilgiler kaybolur 
    â†“
TercÃ¼me kalitesi dÃ¼ÅŸer âŒ
```

**Ã‡Ã¶zÃ¼m:** TÃ¼m gizli durumlarÄ± saklamak (Attention)

#### âš ï¸ Problem #2: SÄ±ralÄ± Ä°ÅŸlem
```
Klasik RNN:
    Word 1 â†’ Word 2 â†’ Word 3 â†’ Word 4
    (Sequential - Bitmesi beklenir)

Transformer:
    Word 1 â”€â”
    Word 2 â”€â”¼â”€â†’ Paralel iÅŸlem
    Word 3 â”€â”¤   (Ã‡ok daha hÄ±zlÄ±!) âš¡
    Word 4 â”€â”˜
```

#### âœ… Dikkat MekanizmasÄ±nÄ±n AvantajlarÄ±

1. **Uzun BaÄŸlamÄ± Tutar**: Paralel olarak tÃ¼m kelimelere eriÅŸim
2. **HÄ±zlÄ± Ä°ÅŸlem**: Paralel hesaplama mÃ¼mkÃ¼n
3. **Esneklik**: FarklÄ± dikkat baÅŸlarÄ± farklÄ± bilgiler yakalar
4. **Yorumlanabilirlik**: Dikkat aÄŸÄ±rlÄ±klarÄ± gÃ¶sterir hangi kelime Ã¶nemli

### ğŸ“Š Performans KÄ±yaslamasÄ±

| Ã–zellik | Seq2Seq | Transformer |
|---------|---------|-------------|
| CÃ¼mle UzunluÄŸu | 20 kelimenin Ã¼zerinde dÃ¼ÅŸer | Uzun cÃ¼mlede stabil |
| Ä°ÅŸlem HÄ±zÄ± | YavaÅŸ (sÄ±ralÄ±) | HÄ±zlÄ± (paralel) |
| Bellek KullanÄ±mÄ± | Az | Daha fazla |
| Yorumlanabilirlik | DÃ¼ÅŸÃ¼k | YÃ¼ksek (dikkat aÄŸÄ±rlÄ±klarÄ±) |
| Benchmarks | BLEU: ~25 | BLEU: ~30+ |

---

## ğŸ“š Kaynaklar ve Referanslar

### ğŸ“– Orijinal Makaleler
1. **"Attention is All You Need"** (Vaswani et al., 2017)
   - Transformer mimarisinin ilk tanÄ±tÄ±mÄ±
   - PDF: https://arxiv.org/abs/1706.03762

2. **"Neural Machine Translation by Jointly Learning to Align and Translate"** (Bahdanau et al., 2014)
   - Attention mekanizmasÄ±nÄ±n ilk versiyonu
   - PDF: https://arxiv.org/abs/1409.0473

### ğŸŒ Online Kaynaklar
- [Hugging Face Transformers DokÃ¼mantasyonu](https://huggingface.co/docs/transformers/)
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [3Blue1Brown: Neural Networks](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)

### ğŸ“š Kitaplar
- "Deep Learning" - Ian Goodfellow, Yoshua Bengio, Aaron Courville
- "Natural Language Processing with Transformers" - Lewis Tunstall, Leandro von Werra, Thomas Wolf

### ğŸ’» Ã–nerilen Kodlama KaynaklarÄ±
- [PyTorch Resmi Tutorial'Ä±](https://pytorch.org/tutorials/)
- [TensorFlow Attention MekanizmasÄ±](https://www.tensorflow.org/text/tutorials/transformer)

---

## ğŸ“‹ HÄ±zlÄ± BaÅŸvuru - Sorular ve Cevaplar

### â“ SÄ±k Sorulan Sorular

**S1: Attention mekanizmasÄ± gerÃ§ekten ne iÅŸe yaradÄ±ÄŸÄ±nÄ± hala anlamadÄ±m?**
> Dikkat, belirsiz kelimeyi cÃ¼mledeki diÄŸer kelimelerle iliÅŸkilendirerek baÄŸlamsal anlamÄ±nÄ± bulan bir mekanizmadÄ±r. Ã–rneÄŸin "Ã§ekti" kelimesi "bankadan" ve "kredi" kelimelerine dikkat ederek "kredi almak" anlamÄ±nÄ± bulur.

**S2: Q, K, V neden Ã¼Ã§ ayrÄ± matris? Neden biri kullanmÄ±yoruz?**
> Ã‡Ã¼nkÃ¼ her birinin farklÄ± rolÃ¼ vardÄ±r:
> - Q: Ne anlamlandÄ±rmak istiyorum?
> - K: Nereyi kontrol etmeliyim?
> - V: GerÃ§ek bilgi nedir?

**S3: âˆšd_k neden tam olarak?**
> VektÃ¶r boyutu arttÄ±kÃ§a nokta Ã§arpÄ±m katlanarak artar. âˆšd_k ile bÃ¶lerek puanlar -1 ile +1 arasÄ±nda tutulur ve softmax stabil kalÄ±r.

**S4: Multi-head attention neden daha iyi?**
> Ã‡Ã¼nkÃ¼ farklÄ± dikkat baÅŸlarÄ± cÃ¼mlenin farklÄ± yÃ¶nlerini yakalar:
> - BaÅŸÄ± 1: Ã–zne-nesne iliÅŸkisine dikkat
> - BaÅŸÄ± 2: Zamansal iliÅŸkilere dikkat
> - BaÅŸÄ± 3: Anlamsal benzerliÄŸe dikkat
> - SonuÃ§: Daha zengin represantasyon

**S5: Bu notebook'u bitirdikten sonra ne yapmalÄ±yÄ±m?**
> Sonraki adÄ±mlar:
> 1. Hugging Face modelleri (BERT, GPT) inceyin
> 2. Fine-tuning uygulamasÄ± yapÄ±n
> 3. GerÃ§ek veri seti ile deneyin
> 4. Kendi modeli eÄŸitmeyi deneyin

---


## ğŸ“œ Lisans

Bu proje MIT lisansÄ± altÄ±nda aÃ§Ä±k kaynak olarak sunulmaktadÄ±r.

---

## â­ TeÅŸekkÃ¼r ve Not

Bu eÄŸitim modÃ¼lleri, NLP ve Transformer mimarisini derinlemesine Ã¶ÄŸrenmek isteyenler iÃ§in tasarlanmÄ±ÅŸtÄ±r. Teorik bilgiyi pratik uygulamalarla birleÅŸtiren bu yaklaÅŸÄ±m, karmaÅŸÄ±k kavramlarÄ± anlaÅŸÄ±lÄ±r kÄ±lmayÄ± hedefler.

**BaÅŸarÄ±lÄ± Ã¶ÄŸrenmeler! ğŸš€**

---

**Son GÃ¼ncelleme:** 15 Ocak 2026  
**ModÃ¼l:** 02 - Transformer Mimarisi ve Temel LLM  
**Seviye:** BaÅŸlangÄ±Ã§ - Orta Seviye
