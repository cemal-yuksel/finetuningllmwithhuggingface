<div align="center">

```text
    â–ˆâ–ˆâ•—  â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—    â–ˆâ–ˆâ•—â–ˆâ–ˆâ•—     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
    â–ˆâ–ˆâ•‘ â–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘    â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â• â–ˆâ–ˆâ•”â•â•â•â•â•
    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â• â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  
    â–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  
    â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â•šâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
    â•šâ•â•  â•šâ•â•â•šâ•â•  â•šâ•â•â•â• â•šâ•â•â•â•â•â•  â•šâ•â•â•â•šâ•â•â• â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•  â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â•â•
                                                                                    
    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ•—     â–ˆâ–ˆâ•—      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—
    â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘
    â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘
    â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘
    â•šâ•â•â•â•â•â• â•šâ•â•â•šâ•â•â•â•â•â•â•   â•šâ•â•   â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•   â•šâ•â•   â•šâ•â• â•šâ•â•â•â•â•â• â•šâ•â•  â•šâ•â•â•â•
```

### ğŸ§  BERT Modellerini DamÄ±tma ile Kompakt ve HÄ±zlÄ± Hale Getirme

**DistilBERT | MobileBERT | TinyBERT - Teorik Derinlemesine Ä°nceleme**

---

[![Python](https://img.shields.io/badge/Python-3.8+-4a7c7e.svg?style=for-the-badge&logo=python&logoColor=white)](https://python.org)
[![Transformers](https://img.shields.io/badge/ğŸ¤—-Transformers-6b5b95.svg?style=for-the-badge)](https://huggingface.co/transformers)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-8b4f5c.svg?style=for-the-badge&logo=pytorch&logoColor=white)](https://pytorch.org)
[![Knowledge Distillation](https://img.shields.io/badge/Knowledge-Distillation-5a7c50.svg?style=for-the-badge)](https://arxiv.org)
[![License](https://img.shields.io/badge/License-Educational-8b7355.svg?style=for-the-badge)](LICENSE)

</div>

---

## ğŸŒŸ Genel BakÄ±ÅŸ

Bu proje, **BERT modellerinin** Ã¼Ã§ farklÄ± **Knowledge Distillation (Bilgi DamÄ±tma)** yaklaÅŸÄ±mÄ±yla nasÄ±l kÃ¼Ã§Ã¼ltÃ¼ldÃ¼ÄŸÃ¼nÃ¼, hÄ±zlandÄ±rÄ±ldÄ±ÄŸÄ±nÄ± ve optimize edildiÄŸini **teorik derinlikle** inceler. YBS (YÃ¶netim BiliÅŸim Sistemleri) perspektifiyle gerÃ§ek dÃ¼nya senaryolarÄ±na odaklanÄ±r.

### ğŸ¯ Proje Ã–zellikleri

<table>
<tr>
<td width="33%" valign="top">

**ğŸ“š DistilBERT**
- âœ… Katman azaltma stratejisi
- âœ… 6 katman (12'den yarÄ±ya)
- âœ… 66M parametre
- âœ… %60 daha hÄ±zlÄ±
- âœ… %40 daha kÃ¼Ã§Ã¼k
- âœ… %97 performans korumasÄ±

</td>
<td width="33%" valign="top">

**ğŸ“± MobileBERT**
- âœ… Inverted Bottleneck yapÄ±sÄ±
- âœ… 24 katman (derinlik korunur)
- âœ… 25M parametre
- âœ… Mobil cihaz optimizasyonu
- âœ… 128 hidden size
- âœ… CPU'da Ã§alÄ±ÅŸabilir

</td>
<td width="33%" valign="top">

**ğŸ¦ TinyBERT**
- âœ… Ä°ki aÅŸamalÄ± damÄ±tma
- âœ… 4 katman (en kÃ¼Ã§Ã¼k)
- âœ… 14M parametre
- âœ… 9.4x daha hÄ±zlÄ±
- âœ… GÃ¶rev-Ã¶zgÃ¼ optimizasyon
- âœ… Ultra kompakt

</td>
</tr>
</table>

### ğŸ­ 3 FarklÄ± YaklaÅŸÄ±m, Tek Hedef: Verimlilik

```mermaid
%%{init: {'theme':'base', 'themeVariables': { 'primaryColor':'#87CEEB','primaryTextColor':'#2d3436','lineColor':'#95a5a6','fontSize':'15px'}}}%%
graph TB
    CENTER((ğŸ§  BERT<br/>110M param<br/>12 katman)) --> DISTIL
    CENTER --> MOBILE
    CENTER --> TINY
    
    subgraph G1["ğŸ“˜ DistilBERT Stratejisi"]
        DISTIL[Katman SayÄ±sÄ±nÄ±<br/>Azalt]
        DISTIL1[12 â†’ 6 katman]
        DISTIL2[Hidden: 768 koru]
        DISTIL3[3 KayÄ±p Fonksiyonu]
        DISTIL --> DISTIL1 --> DISTIL2 --> DISTIL3
    end
    
    subgraph G2["ğŸ“± MobileBERT Stratejisi"]
        MOBILE[Katman GeniÅŸliÄŸini<br/>Azalt]
        MOBILE1[24 katman]
        MOBILE2[Hidden: 128]
        MOBILE3[Inverted Bottleneck]
        MOBILE --> MOBILE1 --> MOBILE2 --> MOBILE3
    end
    
    subgraph G3["ğŸ¦ TinyBERT Stratejisi"]
        TINY[Ä°ki AÅŸamalÄ±<br/>DamÄ±tma]
        TINY1[4 katman]
        TINY2[Hidden: 312]
        TINY3[GÃ¶rev-Ã–zgÃ¼]
        TINY --> TINY1 --> TINY2 --> TINY3
    end
    
    DISTIL3 --> RESULT[ğŸ¯ SonuÃ§:<br/>HÄ±zlÄ±, Kompakt,<br/>Etkili Modeller]
    MOBILE3 --> RESULT
    TINY3 --> RESULT
    
    style CENTER fill:#4169E1,stroke:#1E90FF,stroke-width:5px,color:#ffffff
    
    style G1 fill:#E8F4F8,stroke:#B0D4E3,stroke-width:2px
    style G2 fill:#FFE5EC,stroke:#FFB6C1,stroke-width:2px
    style G3 fill:#FFF9E6,stroke:#FFE5B4,stroke-width:2px
    
    style DISTIL fill:#87CEEB,stroke:#6495ED,stroke-width:3px,color:#2d3436
    style MOBILE fill:#FFB6C1,stroke:#FF69B4,stroke-width:3px,color:#2d3436
    style TINY fill:#FFD700,stroke:#FFA500,stroke-width:3px,color:#2d3436
    
    style RESULT fill:#98FB98,stroke:#32CD32,stroke-width:4px,color:#2d3436
```

---

## ğŸ“Š Model Performans KarÅŸÄ±laÅŸtÄ±rmasÄ±

### ğŸ¯ Teknik Ã–zellikler

<table>
<tr>
<td align="center" width="20%">
<div>
<h3>ğŸ”µ</h3>
<h2>BERT-Base</h2>
<p><b>Baseline</b></p>
</div>
</td>
<td align="center" width="20%">
<div>
<h3>ğŸ“˜</h3>
<h2>DistilBERT</h2>
<p><b>Katman Azaltma</b></p>
</div>
</td>
<td align="center" width="20%">
<div>
<h3>ğŸ“±</h3>
<h2>MobileBERT</h2>
<p><b>GeniÅŸlik Azaltma</b></p>
</div>
</td>
<td align="center" width="20%">
<div>
<h3>ğŸ¦</h3>
<h2>TinyBERT</h2>
<p><b>Ä°ki AÅŸamalÄ±</b></p>
</div>
</td>
</tr>
</table>

| Metrik | BERT-Base | DistilBERT | MobileBERT | TinyBERT |
|--------|-----------|------------|------------|----------|
| **Parametre SayÄ±sÄ±** | 110M | 66M | 25M | 14M |
| **Katman SayÄ±sÄ±** | 12 | 6 | 24 | 4 |
| **Hidden Size** | 768 | 768 | 128 | 312 |
| **Attention Heads** | 12 | 12 | 4 | 12 |
| **HÄ±z (Relative)** | 1.0x | 1.6x | 4.0x | 9.4x |
| **Model Boyutu** | ~440MB | ~260MB | ~100MB | ~60MB |
| **GLUE Score** | 79.6 | 77.0 | 77.7 | 74.5 |
| **SQuAD F1** | 88.5 | 86.9 | 90.0 | 82.1 |
| **Performans KorumasÄ±** | 100% | 97% | 96% | 93% |

### âš™ï¸ Deployment SenaryolarÄ±

| Senaryo | BERT-Base | DistilBERT | MobileBERT | TinyBERT |
|---------|-----------|------------|------------|----------|
| **Bulut Sunucu (GPU)** | âœ…âœ…âœ… | âœ…âœ…âœ… | âœ…âœ… | âœ… |
| **Edge Computing** | âš ï¸ | âœ…âœ… | âœ…âœ…âœ… | âœ…âœ… |
| **Mobil Cihaz** | âŒ | âš ï¸ | âœ…âœ…âœ… | âœ…âœ…âœ… |
| **IoT CihazlarÄ±** | âŒ | âŒ | âš ï¸ | âœ… |
| **Offline KullanÄ±m** | âš ï¸ | âœ… | âœ…âœ…âœ… | âœ…âœ…âœ… |
| **GerÃ§ek ZamanlÄ±** | âš ï¸ | âœ… | âœ…âœ… | âœ…âœ…âœ… |

---

## ğŸ“‚ Proje YapÄ±sÄ±

```
05.knowledge-distillation-distilbert_tinybert_mobilebert/
â”‚
â”œâ”€â”€ ğŸ““ 01.distilbert.ipynb          # DistilBERT teorik inceleme
â”œâ”€â”€ ğŸ““ 02.mobilebert.ipynb          # MobileBERT teorik inceleme
â”œâ”€â”€ ğŸ““ 03.tinybert.ipynb            # TinyBERT teorik inceleme
â”œâ”€â”€ ğŸ“„ README.md                     # Bu dosya
â””â”€â”€ ğŸ“„ ornREADME.md                  # Referans Ã¶rnek dosya
```

---

## ğŸ“˜ Model 1: DistilBERT - Katman Azaltma Stratejisi

### ğŸ¯ Ana Konsept

**DistilBERT**, BERT-Base'in katman sayÄ±sÄ±nÄ± **yarÄ±ya indirerek** (12 â†’ 6) kÃ¼Ã§Ã¼ltme ve hÄ±zlandÄ±rma saÄŸlar. **SÄ±ÄŸ ama geniÅŸ** bir mimari yaklaÅŸÄ±mÄ± benimser.


### ğŸ“ ÃœÃ§ KayÄ±p Fonksiyonu

DistilBERT, Ã¶ÄŸretmenden bilgi transfer ederken **3 farklÄ± kayÄ±p fonksiyonu** kullanÄ±r:

#### 1ï¸âƒ£ **Distillation Loss (DamÄ±tma KaybÄ±)**

Ã–ÄŸretmenin soft probability daÄŸÄ±lÄ±mÄ±nÄ± taklit eder.

$$
\mathcal{L}_{\text{distil}} = -\sum_{i} p_i^{\text{teacher}} \log(p_i^{\text{student}})
$$

**Temperature Scaling:**

$$
p_i = \frac{\exp(z_i / T)}{\sum_j \exp(z_j / T)}
$$

- **T = 1:** Normal softmax (keskin kararlar)
- **T > 1:** YumuÅŸak daÄŸÄ±lÄ±m (sÄ±nÄ±flar arasÄ± iliÅŸkiler gÃ¶rÃ¼nÃ¼r)

#### 2ï¸âƒ£ **Student Loss (Ã–ÄŸrenci KaybÄ±)**

GerÃ§ek etiketlerle standard cross-entropy:

$$
\mathcal{L}_{\text{student}} = -\sum_{i} y_i \log(\hat{y}_i)
$$

#### 3ï¸âƒ£ **Cosine Embedding Loss (KosinÃ¼s KayÄ±p)**

Hidden state'lerin yÃ¶nlerini hizalar:

$$
\mathcal{L}_{\text{cosine}} = 1 - \frac{h^{\text{teacher}} \cdot h^{\text{student}}}{\|h^{\text{teacher}}\| \|h^{\text{student}}\|}
$$

**Toplam KayÄ±p:**

$$
\mathcal{L}_{\text{total}} = \alpha \mathcal{L}_{\text{distil}} + \beta \mathcal{L}_{\text{student}} + \gamma \mathcal{L}_{\text{cosine}}
$$

### ğŸ¯ Avantajlar ve KullanÄ±m AlanlarÄ±

âœ… **Avantajlar:**
- %60 daha hÄ±zlÄ± inference
- %40 daha kÃ¼Ã§Ã¼k model boyutu
- %97 performans korumasÄ±
- Genel amaÃ§lÄ± kullanÄ±m
- Kolay fine-tuning

âš ï¸ **Trade-offs:**
- Daha sÄ±ÄŸ mimari (6 katman)
- Kompleks anlam Ã§Ä±karÄ±mÄ±nda BERT'e gÃ¶re zayÄ±f
- NSP (Next Sentence Prediction) gÃ¶revi yok

ğŸ’¼ **Ä°deal KullanÄ±m:**
- API servisleri
- GerÃ§ek zamanlÄ± sÄ±nÄ±flandÄ±rma
- Bulut tabanlÄ± sistemler
- Orta Ã¶lÃ§ekli deployment

---

## ğŸ“± Model 2: MobileBERT - Inverted Bottleneck ile GeniÅŸlik Optimizasyonu

### ğŸ¯ Ana Konsept

**MobileBERT**, katman sayÄ±sÄ±nÄ± korurken (24 katman) **hidden size'Ä± drastik olarak azaltarak** (768 â†’ 128) kÃ¼Ã§Ã¼ltme saÄŸlar. **Derin ama dar** bir mimari yaklaÅŸÄ±mÄ± benimser. Mobil cihazlar ve edge computing iÃ§in optimize edilmiÅŸtir.

### ğŸ­ Inverted Bottleneck: Dar GiriÅŸ, Zengin Ä°ÅŸlem, Dar Ã‡Ä±kÄ±ÅŸ

```mermaid
%%{init: {'theme':'base', 'themeVariables': { 'primaryColor':'#FFB6C1','primaryTextColor':'#2d3436','lineColor':'#95a5a6','fontSize':'14px'}}}%%
graph TB
    START["ğŸ¯ MobileBERT Encoder Layer"] --> IN["ğŸ“¥ GiriÅŸ<br/>128 dim<br/>Kompakt"]
    
    IN --> BN1["ğŸ”¼ Bottleneck 1<br/>128 â†’ 512<br/>GENÄ°ÅLET"]
    BN1 --> ATT["ğŸ” Multi-Head<br/>Attention<br/>512 dim<br/>4 heads"]
    ATT --> BN2["ğŸ”½ Bottleneck 2<br/>512 â†’ 128<br/>DARALT"]
    
    BN2 --> ADD1["â• Add & Norm<br/>128 dim"]
    IN -.->|Skip Connection| ADD1
    
    ADD1 --> BN3["ğŸ”¼ Bottleneck 3<br/>128 â†’ 512<br/>GENÄ°ÅLET"]
    BN3 --> FFN["ğŸ”„ Stacked FFN<br/>512â†’512<br/>Ã—4 katman"]
    FFN --> BN4["ğŸ”½ Bottleneck 4<br/>512 â†’ 128<br/>DARALT"]
    
    BN4 --> ADD2["â• Add & Norm<br/>128 dim"]
    ADD1 -.->|Skip Connection| ADD2
    
    ADD2 --> OUT["ğŸ“¤ Ã‡Ä±kÄ±ÅŸ<br/>128 dim<br/>Kompakt"]
    
    style START fill:#FFE5EC,stroke:#FF69B4,stroke-width:3px,color:#2d3436
    style IN fill:#C8F5E3,stroke:#00D084,stroke-width:2px,color:#2d3436
    style BN1 fill:#FFE5CC,stroke:#FF9933,stroke-width:2px,color:#2d3436
    style ATT fill:#E5E5FF,stroke:#6B6BFF,stroke-width:3px,color:#2d3436
    style BN2 fill:#FFE5CC,stroke:#FF9933,stroke-width:2px,color:#2d3436
    style ADD1 fill:#FFF5CC,stroke:#FFD700,stroke-width:2px,color:#2d3436
    style BN3 fill:#FFE5CC,stroke:#FF9933,stroke-width:2px,color:#2d3436
    style FFN fill:#FFD6E8,stroke:#FF6B9D,stroke-width:3px,color:#2d3436
    style BN4 fill:#FFE5CC,stroke:#FF9933,stroke-width:2px,color:#2d3436
    style ADD2 fill:#FFF5CC,stroke:#FFD700,stroke-width:2px,color:#2d3436
    style OUT fill:#C8F5E3,stroke:#00D084,stroke-width:2px,color:#2d3436
```

### ğŸ”‘ Anahtar Ã–zellikler

#### 1ï¸âƒ£ **Inverted Bottleneck YapÄ±sÄ±**

**Geleneksel vs Inverted:**

```
âŒ Geleneksel Bottleneck:
GeniÅŸ (768) â†’ DAR (256) â†’ GeniÅŸ (768)
â””â”€ Bilgi kaybÄ± oluÅŸur

âœ… Inverted Bottleneck:
DAR (128) â†’ GeniÅŸ (512) â†’ DAR (128)
â””â”€ Kompakt iletiÅŸim + Zengin iÅŸlem
```

**Avantajlar:**
- GiriÅŸ/Ã‡Ä±kÄ±ÅŸ hafif (128 dim) â†’ Bellek â†“
- Ä°ÅŸlem zengin (512 dim) â†’ Temsil gÃ¼cÃ¼ â†‘
- Skip connection verimli â†’ Gradient akÄ±ÅŸÄ± iyi

#### 2ï¸âƒ£ **Stacked Feed-Forward Networks**

Her encoder layer'da **4 adet FFN** sÄ±rayla Ã§alÄ±ÅŸÄ±r:

$$
\text{FFN}_{\text{stacked}} = \text{FFN}_4 \circ \text{FFN}_3 \circ \text{FFN}_2 \circ \text{FFN}_1
$$

Bu yapÄ±, az parametre ile yÃ¼ksek temsil gÃ¼cÃ¼ saÄŸlar.

#### 3ï¸âƒ£ **Teacher Model: IB-BERT**

MobileBERT, **IB-BERT (Inverted-Bottleneck BERT)** adlÄ± Ã¶zel bir teacher modelinden Ã¶ÄŸrenir:

- Teacher da inverted bottleneck kullanÄ±r
- Student ile aynÄ± mimari, daha geniÅŸ
- 4 tÃ¼r bilgi transferi yapÄ±lÄ±r

### ğŸ“Š Transfer MekanizmalarÄ±

| Transfer TÃ¼rÃ¼ | Ne AktarÄ±lÄ±r | AmaÃ§ |
|---------------|--------------|------|
| **Attention Transfer** | Dikkat matrisleri | Hangi kelimeye odaklanmalÄ± |
| **Feature Map Transfer** | Hidden states | Ara katman temsilleri |
| **Embedding Transfer** | Token embeddings | Kelime vektÃ¶rleri |
| **Pre-training Transfer** | MLM Ã§Ä±ktÄ±larÄ± | Genel dil bilgisi |

### ğŸ¯ Avantajlar ve KullanÄ±m AlanlarÄ±

âœ… **Avantajlar:**
- 25M parametre (BERT'in %23'Ã¼)
- 4x daha hÄ±zlÄ±
- Mobil cihazlarda Ã§alÄ±ÅŸabilir
- 24 katman (derinlik korunur)
- CPU'da bile verimli

âš ï¸ **Trade-offs:**
- Dar hidden size (128)
- KarmaÅŸÄ±k transfer sÃ¼reci
- Ã–zel teacher modeli gerekir

ğŸ’¼ **Ä°deal KullanÄ±m:**
- Mobil uygulamalar
- Edge computing
- IoT cihazlarÄ±
- Offline sistemler
- GerÃ§ek zamanlÄ± inference

---

## ğŸ¦ Model 3: TinyBERT - Ä°ki AÅŸamalÄ± DamÄ±tma Stratejisi

### ğŸ¯ Ana Konsept

**TinyBERT**, bilgi damÄ±tmasÄ±nÄ± **iki ayrÄ± aÅŸamada** gerÃ§ekleÅŸtiren benzersiz bir yaklaÅŸÄ±mdÄ±r. Ä°lk aÅŸamada genel dil bilgisi, ikinci aÅŸamada ise gÃ¶reve Ã¶zel bilgi aktarÄ±lÄ±r.

### ğŸ­ Ä°ki AÅŸamalÄ± Ã–ÄŸrenme: Genel EÄŸitim + GÃ¶rev EÄŸitimi

```mermaid
%%{init: {'theme':'base', 'themeVariables': { 'primaryColor':'#FFD700','primaryTextColor':'#2d3436','lineColor':'#95a5a6','fontSize':'14px'}}}%%
graph TB
    START["ğŸ“ TinyBERT EÄŸitim SÃ¼reci"] --> PHASE1
    
    subgraph PHASE1["ğŸ“š AÅAMA 1: Genel Dil EÄŸitimi"]
        T1["ğŸ‘¨â€ğŸ« Teacher BERT<br/>Pretrained"]
        S1["ğŸ‘¨â€ğŸ“ Student TinyBERT<br/>Random Init"]
        DATA1["ğŸ“Š Genel Corpus<br/>Wikipedia, Books"]
        
        T1 --> TRANS1["ğŸ”„ Transfer:<br/>â€¢ Embedding<br/>â€¢ Attention<br/>â€¢ Hidden States"]
        DATA1 --> TRANS1
        TRANS1 --> S1
    end
    
    PHASE1 --> PHASE2
    
    subgraph PHASE2["ğŸ¯ AÅAMA 2: GÃ¶rev-Ã–zgÃ¼ EÄŸitim"]
        T2["ğŸ‘¨â€ğŸ« Teacher BERT<br/>Fine-tuned"]
        S2["ğŸ‘¨â€ğŸ“ Student TinyBERT<br/>AÅŸama 1'den"]
        DATA2["ğŸ“Š GÃ¶rev Verisi<br/>GLUE, SQuAD"]
        
        T2 --> TRANS2["ğŸ”„ Transfer:<br/>â€¢ Embedding<br/>â€¢ Attention<br/>â€¢ Hidden States<br/>â€¢ Prediction"]
        DATA2 --> TRANS2
        TRANS2 --> S2
    end
    
    PHASE2 --> RESULT["âœ… SonuÃ§:<br/>GÃ¶rev-Ã–zgÃ¼<br/>Optimize TinyBERT"]
    
    style START fill:#FFF9E6,stroke:#FFA500,stroke-width:3px,color:#2d3436
    style PHASE1 fill:#E8F4F8,stroke:#87CEEB,stroke-width:2px
    style PHASE2 fill:#FFE5EC,stroke:#FFB6C1,stroke-width:2px
    style T1 fill:#FFE5CC,stroke:#FF9933,stroke-width:2px,color:#2d3436
    style S1 fill:#C8F5E3,stroke:#00D084,stroke-width:2px,color:#2d3436
    style T2 fill:#FFE5CC,stroke:#FF9933,stroke-width:2px,color:#2d3436
    style S2 fill:#C8F5E3,stroke:#00D084,stroke-width:2px,color:#2d3436
    style RESULT fill:#98FB98,stroke:#32CD32,stroke-width:4px,color:#2d3436
```

### ğŸ”‘ 4 Seviyeli Bilgi Transferi

TinyBERT, modelin her seviyesinden bilgi transfer eder:

#### 1ï¸âƒ£ **Embedding Layer Transfer**

Token, positional ve segment embedding'leri aktarÄ±r:

$$
\mathcal{L}_{\text{embd}} = \text{MSE}(W_e \cdot E^S, E^T)
$$

- $E^S$: Student embeddings
- $E^T$: Teacher embeddings
- $W_e$: Boyut eÅŸleÅŸtirme matrisi

#### 2ï¸âƒ£ **Hidden States Transfer**

Her katmanÄ±n Ã§Ä±ktÄ± tensÃ¶rlerini hizalar:

$$
\mathcal{L}_{\text{hidn}} = \text{MSE}(W_h \cdot H^S, H^T)
$$

#### 3ï¸âƒ£ **Attention Transfer**

Multi-head attention matrislerini kopyalar:

$$
\mathcal{L}_{\text{attn}} = \frac{1}{h} \sum_{i=1}^{h} \text{MSE}(A_i^S, A_i^T)
$$

- $h$: Attention head sayÄ±sÄ±
- $A_i$: i'nci head'in attention matrisi

#### 4ï¸âƒ£ **Prediction Layer Transfer**

Final softmax Ã§Ä±ktÄ±larÄ±nÄ± yumuÅŸatÄ±r:

$$
\mathcal{L}_{\text{pred}} = -\sum_{i} \text{softmax}(z_i^T / t) \cdot \log \text{softmax}(z_i^S / t)
$$

**Toplam KayÄ±p:**

$$
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{embd}} + \mathcal{L}_{\text{hidn}} + \mathcal{L}_{\text{attn}} + \mathcal{L}_{\text{pred}}
$$

### ğŸ¯ Avantajlar ve KullanÄ±m AlanlarÄ±

âœ… **Avantajlar:**
- En kÃ¼Ã§Ã¼k model (14M parametre)
- 9.4x daha hÄ±zlÄ±
- CPU'da mÃ¼kemmel performans
- GÃ¶reve Ã¶zel optimizasyon
- Ä°ki aÅŸamalÄ± Ã¶ÄŸrenme ile yÃ¼ksek kalite

âš ï¸ **Trade-offs:**
- Sadece 4 katman (en sÄ±ÄŸ)
- KarmaÅŸÄ±k eÄŸitim sÃ¼reci
- Her gÃ¶rev iÃ§in ayrÄ± model
- EÄŸitim maliyeti yÃ¼ksek

ğŸ’¼ **Ä°deal KullanÄ±m:**
- Toplu iÅŸlem sistemleri
- Ã‡aÄŸrÄ± merkezi analitiÄŸi
- Sosyal medya izleme
- CPU tabanlÄ± deployment
- YÃ¼ksek throughput gereken sistemler

---

## ğŸ’¼ YBS Ä°ÅŸ SenaryolarÄ±: GerÃ§ek DÃ¼nya UygulamalarÄ±

### ğŸ­ Model SeÃ§im Karar Matrisi

```mermaid
%%{init: {'theme':'base', 'themeVariables': { 'primaryColor':'#87CEEB','primaryTextColor':'#2d3436','lineColor':'#95a5a6','fontSize':'14px'}}}%%
graph TD
    START["ğŸ¤” Hangi Model?"] --> Q1{"Deployment<br/>OrtamÄ±?"}
    
    Q1 -->|Bulut/Sunucu| Q2{"HÄ±z vs<br/>DoÄŸruluk?"}
    Q1 -->|Mobil/Edge| Q3{"Cihaz<br/>GÃ¼cÃ¼?"}
    Q1 -->|CPU Only| TINY["ğŸ¦ TinyBERT<br/>9.4x hÄ±zlÄ±<br/>CPU'da mÃ¼kemmel"]
    
    Q2 -->|Dengeli| DISTIL["ğŸ“˜ DistilBERT<br/>%97 doÄŸruluk<br/>%60 hÄ±zlÄ±"]
    Q2 -->|Maksimum HÄ±z| TINY
    
    Q3 -->|Orta/Ä°yi| MOBILE["ğŸ“± MobileBERT<br/>Mobil optimize<br/>Offline Ã§alÄ±ÅŸÄ±r"]
    Q3 -->|Ã‡ok ZayÄ±f| TINY
    
    style START fill:#FFE5E5,stroke:#FF6B6B,stroke-width:3px,color:#2d3436
    style DISTIL fill:#E8F4F8,stroke:#87CEEB,stroke-width:3px,color:#2d3436
    style MOBILE fill:#FFE5EC,stroke:#FFB6C1,stroke-width:3px,color:#2d3436
    style TINY fill:#FFF9E6,stroke:#FFD700,stroke-width:3px,color:#2d3436
```

### ğŸ¦ Senaryo 1: Banka MÃ¼ÅŸteri Destek Sistemi

**Durum:** GÃ¼nde 50,000 mÃ¼ÅŸteri mesajÄ± gelir, otomatik kategorize edilmeli.

| Gereksinim | DistilBERT | MobileBERT | TinyBERT |
|------------|------------|------------|----------|
| **DoÄŸruluk (%95+)** | âœ…âœ… %97 | âœ… %96 | âš ï¸ %93 |
| **Gecikme (<100ms)** | âœ… 55ms | âœ…âœ… 45ms | âœ…âœ… 16ms |
| **Maliyet** | Orta | DÃ¼ÅŸÃ¼k | Ã‡ok DÃ¼ÅŸÃ¼k |
| **SonuÃ§** | âœ… Ä°deal | âœ… Ä°yi | âš ï¸ DoÄŸruluk sÄ±nÄ±rda |

**Karar:** **DistilBERT** - DoÄŸruluk kritik, performans yeterli.

### ğŸ“± Senaryo 2: Mobil BankacÄ±lÄ±k UygulamasÄ±

**Durum:** KullanÄ±cÄ± mesaj yazÄ±yor, app offline Ã§alÄ±ÅŸabilmeli.

| Gereksinim | DistilBERT | MobileBERT | TinyBERT |
|------------|------------|------------|----------|
| **Model Boyutu (<100MB)** | âš ï¸ 260MB | âœ…âœ… 100MB | âœ…âœ… 60MB |
| **Batarya Dostu** | âš ï¸ Orta | âœ…âœ… Ä°yi | âœ…âœ… MÃ¼kemmel |
| **Offline Ã‡alÄ±ÅŸma** | âœ… Olur | âœ…âœ… Optimize | âœ…âœ… Optimize |
| **SonuÃ§** | âŒ BÃ¼yÃ¼k | âœ…âœ… MÃ¼kemmel | âœ… Ä°yi |

**Karar:** **MobileBERT** - Mobil iÃ§in Ã¶zel tasarlandÄ±, boyut ideal.

### ğŸ¢ Senaryo 3: E-Ticaret Toplu Yorum Analizi

**Durum:** Gece 200,000 yorumu toplu iÅŸle, CPU Ã¼zerinde.

| Gereksinim | DistilBERT | MobileBERT | TinyBERT |
|------------|------------|------------|----------|
| **CPU PerformansÄ±** | âš ï¸ YavaÅŸ | âœ… Ä°yi | âœ…âœ… MÃ¼kemmel |
| **Throughput** | Orta | YÃ¼ksek | Ã‡ok YÃ¼ksek |
| **Maliyet (CPU)** | YÃ¼ksek | Orta | DÃ¼ÅŸÃ¼k |
| **SonuÃ§** | âŒ Maliyetli | âœ… Ä°yi | âœ…âœ… Ä°deal |

**Karar:** **TinyBERT** - CPU'da 9.4x hÄ±zlÄ±, maliyet minimuma iner.

### ğŸ“Š ROI Analizi: YÄ±llÄ±k Maliyet KarÅŸÄ±laÅŸtÄ±rmasÄ±

**Senaryo:** GÃ¼nlÃ¼k 100K istek, 7/24 Ã§alÄ±ÅŸan sistem

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  â”‚  BERT-Base   â”‚  DistilBERT  â”‚  MobileBERT  â”‚  TinyBERT    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Sunucu Tipi      â”‚  GPU (T4)    â”‚  GPU (T4)    â”‚  CPU         â”‚  CPU         â”‚
â”‚ AylÄ±k Maliyet    â”‚  $1,200      â”‚  $500        â”‚  $250        â”‚  $150        â”‚
â”‚ YÄ±llÄ±k           â”‚  $14,400     â”‚  $6,000      â”‚  $3,000      â”‚  $1,800      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Tasarruf         â”‚  Baseline    â”‚  $8,400      â”‚  $11,400     â”‚  $12,600     â”‚
â”‚ (BERT'e gÃ¶re)    â”‚              â”‚  (58%)       â”‚  (79%)       â”‚  (88%)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Kurulum ve KullanÄ±m

### ğŸ“¦ Gereksinimler

```bash
Python 3.8+
transformers>=4.30.0
torch>=2.0.0
numpy>=1.24.0
```

### ğŸ’» HÄ±zlÄ± BaÅŸlangÄ±Ã§

#### DistilBERT KullanÄ±mÄ±

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# Model ve tokenizer yÃ¼kleme
model_name = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# Tahmin
text = "Bu Ã¼rÃ¼n harika!"
inputs = tokenizer(text, return_tensors="pt")
outputs = model(**inputs)
```

#### MobileBERT KullanÄ±mÄ±

```python
from transformers import MobileBertTokenizer, MobileBertForSequenceClassification

# Model yÃ¼kleme
tokenizer = MobileBertTokenizer.from_pretrained("google/mobilebert-uncased")
model = MobileBertForSequenceClassification.from_pretrained("google/mobilebert-uncased")

# Inference
text = "HÄ±zlÄ± ve verimli!"
inputs = tokenizer(text, return_tensors="pt")
outputs = model(**inputs)
```

#### TinyBERT KullanÄ±mÄ±

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# Model yÃ¼kleme
tokenizer = AutoTokenizer.from_pretrained("huawei-noah/TinyBERT_General_4L_312D")
model = AutoModelForSequenceClassification.from_pretrained("huawei-noah/TinyBERT_General_4L_312D")

# CPU'da Ã§alÄ±ÅŸtÄ±rma
model.eval()
with torch.no_grad():
    inputs = tokenizer(text, return_tensors="pt")
    outputs = model(**inputs)
```

---

## ğŸ“š Ã–ÄŸrenme KaynaklarÄ±

### ğŸ“„ Orijinal Makaleler

1. **DistilBERT** (2019)
   - *DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter*
   - Victor Sanh et al., Hugging Face
   - [arXiv:1910.01108](https://arxiv.org/abs/1910.01108)

2. **MobileBERT** (2020)
   - *MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices*
   - Zhiqing Sun et al., CMU & Google
   - [arXiv:2004.02984](https://arxiv.org/abs/2004.02984)

3. **TinyBERT** (2020)
   - *TinyBERT: Distilling BERT for Natural Language Understanding*
   - Xiaoqi Jiao et al., Huawei Noah's Ark Lab
   - [arXiv:1909.10351](https://arxiv.org/abs/1909.10351)

### ğŸ”— FaydalÄ± Linkler

- [Hugging Face Model Hub](https://huggingface.co/models)
- [BERT Paper](https://arxiv.org/abs/1810.04805)
- [Knowledge Distillation Survey](https://arxiv.org/abs/2006.05525)
- [Transformers Documentation](https://huggingface.co/docs/transformers)

---

## ğŸ“ Ã–ÄŸrenme Hedefleri

Bu projeyi tamamladÄ±ÄŸÄ±nÄ±zda:

âœ… **Knowledge Distillation** kavramÄ±nÄ± derinlemesine anlayacaksÄ±nÄ±z  
âœ… **Teacher-Student** paradigmasÄ±nÄ±n matematiksel temellerini kavrayacaksÄ±nÄ±z  
âœ… **3 farklÄ± damÄ±tma stratejisini** karÅŸÄ±laÅŸtÄ±rabileceksiniz  
âœ… **Model seÃ§imi** iÃ§in YBS perspektifiyle karar verebileceksiniz  
âœ… **Performans-Maliyet** dengesini analiz edebileceksiniz  
âœ… **Deployment senaryolarÄ±na** gÃ¶re doÄŸru modeli seÃ§ebileceksiniz  
âœ… **Ä°ÅŸ dÃ¼nyasÄ± uygulamalarÄ±nda** pratik Ã§Ã¶zÃ¼mler Ã¼retebileceksiniz  

---

## ğŸ¤ KatkÄ±da Bulunma

Bu proje eÄŸitim amaÃ§lÄ±dÄ±r. Ã–nerileriniz iÃ§in issue aÃ§abilir veya pull request gÃ¶nderebilirsiniz.

---

## ğŸ“œ Lisans

Bu proje eÄŸitim amaÃ§lÄ± hazÄ±rlanmÄ±ÅŸtÄ±r. Kaynak gÃ¶stererek kullanabilirsiniz.

---

<div align="center">

### ğŸŒŸ BaÅŸarÄ±lar Dileriz! ğŸŒŸ

**"En iyi model, en doÄŸru olanÄ± deÄŸil; iÅŸletme hedeflerine en uygun olanÄ±dÄ±r."**

---

Made with â¤ï¸ by Cemal YÃœKSEL for YBS Students

</div>

