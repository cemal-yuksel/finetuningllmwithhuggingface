<div align="center">

```text
 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
 â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â•šâ•â•â–ˆâ–ˆâ•”â•â•â•
 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•   â–ˆâ–ˆâ•‘   
 â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•‘   
 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   
 â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•   â•šâ•â•   
                                   
   Architecture Theory & Deep Dive
```



### ğŸ“ BERT Mimari Teori ve Derin Analiz

**Bidirectional Encoder Representations from Transformers**  
*DoÄŸal Dil Ä°ÅŸleme'de Devrim Yaratan Mimari*

---

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg?style=flat&logo=python)](https://python.org)
[![Transformers](https://img.shields.io/badge/ğŸ¤—-Transformers-yellow.svg)](https://huggingface.co/transformers)
[![License](https://img.shields.io/badge/License-Educational-green.svg)](LICENSE)
[![Status](https://img.shields.io/badge/Status-Active-success.svg)](https://github.com)

</div>

---

## ğŸŒŸ Genel BakÄ±ÅŸ

Bu klasÃ¶r, **BERT (Bidirectional Encoder Representations from Transformers)** mimarisinin derinlemesine incelenmesini iÃ§eren kapsamlÄ± eÄŸitim materyallerini barÄ±ndÄ±rmaktadÄ±r. Modern NLP'nin temel taÅŸlarÄ±ndan biri olan BERT'in teorik temellerinden pratik uygulamalarÄ±na kadar her ÅŸeyi Ã¶ÄŸreneceksiniz.

### ğŸ¯ Bu ModÃ¼l Size Neler KazandÄ±racak?

<table>
<tr>
<td width="50%">

**ğŸ“š Teorik Derinlik**
- âœ… BaÄŸlam (Context) kavramÄ±nÄ±n temelleri
- âœ… Ã‡ift yÃ¶nlÃ¼ (Bidirectional) anlama felsefesi
- âœ… Transformer mimarisinin BERT'e uyarlanmasÄ±
- âœ… Self-Attention mekanizmasÄ±nÄ±n gÃ¼cÃ¼

</td>
<td width="50%">

**ğŸ› ï¸ Pratik Uygulama**
- âœ… WordPiece tokenization detaylarÄ±
- âœ… Embedding katmanlarÄ±nÄ±n yapÄ±sÄ±
- âœ… MLM ve NSP eÄŸitim stratejileri
- âœ… Fine-tuning ve transfer learning

</td>
</tr>
</table>

### ğŸ“ Hedef Kitle

- ğŸ“– **Yeni BaÅŸlayanlar:** HiÃ§ NLP bilmeyenler iÃ§in sÄ±fÄ±rdan anlatÄ±m
- ğŸš€ **Orta Seviye:** Transformer kavramlarÄ±nÄ± pekiÅŸtirmek isteyenler
- ğŸ”¬ **Ä°leri DÃ¼zey:** BERT'in iÃ§ mekaniklerini anlamak isteyenler
- ğŸ’¼ **YBS Ã–ÄŸrencileri:** GerÃ§ek dÃ¼nya uygulamalarÄ± ve iÅŸ senaryolarÄ±

---

## ï¿½ Ä°Ã§indekiler

- [ğŸŒŸ Genel BakÄ±ÅŸ](#-genel-bakÄ±ÅŸ)
- [ğŸ“‚ KlasÃ¶r Ä°Ã§eriÄŸi](#-klasÃ¶r-iÃ§eriÄŸi)
  - [ğŸ““ Notebook 1: BERT Paper & Terminology](#-notebook-1-bert-paper--terminology)
  - [ğŸ““ Notebook 2: WordPiece & Training Fundamentals](#-notebook-2-wordpiece--training-fundamentals)
- [ğŸ¨ BERT Mimarisi GÃ¶rselleÅŸtirme](#-bert-mimarisi-gÃ¶rselleÅŸtirme)
- [ğŸ”„ Tokenization Pipeline](#-tokenization-pipeline)
- [ğŸ“ Training Pipeline](#-training-pipeline)
- [ğŸš€ NasÄ±l KullanÄ±lÄ±r](#-nasÄ±l-kullanÄ±lÄ±r)
- [ğŸ“š Ã–ÄŸrenme Yol HaritasÄ±](#-Ã¶ÄŸrenme-yol-haritasÄ±)
- [âš™ï¸ Gereksinimler](#ï¸-gereksinimler)
- [ğŸ’¡ Ä°puÃ§larÄ±](#-iÌ‡puÃ§larÄ±)
- [ğŸ“– Kaynaklar](#-kaynaklar)

---

## ğŸ“‚ KlasÃ¶r Ä°Ã§eriÄŸi

Bu klasÃ¶rde **2 temel notebook** bulunmaktadÄ±r:

### ğŸ““ Notebook 1: BERT Paper & Terminology

**Dosya:** `01.bertpaper-terminology.ipynb`  
**SÃ¼re:** ~120 dakika  
**Seviye:** ğŸŸ¢ BaÅŸlangÄ±Ã§ â†’ ğŸŸ¡ Orta

#### ğŸ¯ Ne Ã–ÄŸreneceksiniz?

<div align="left">

```mermaid
%%{init: {'theme':'base', 'themeVariables': { 'primaryColor':'#FFE5E5','primaryTextColor':'#2C3E50','primaryBorderColor':'#FF9999','lineColor':'#FFB6C1','secondaryColor':'#E5F3FF','tertiaryColor':'#FFF9E5'}}}%%
mindmap
  root((BERT Terminoloji))
    Context KavramÄ±
      Ã‡ok AnlamlÄ±lÄ±k
      BaÄŸlam Penceresi
      TÃ¼rkÃ§e Ã–rnekler
    Tek vs Ã‡ift YÃ¶nlÃ¼
      Unidirectional
      Bidirectional
      Film Metaforu
    GPT vs BERT
      KarÅŸÄ±laÅŸtÄ±rma
      KullanÄ±m AlanlarÄ±
    Transformer
      Self-Attention
      Encoder-Decoder
    EÄŸitim YÃ¶ntemleri
      MLM
      NSP
```

</div>

#### ğŸ“‹ BÃ¶lÃ¼m DetaylarÄ±

| AdÄ±m | Konu | AÃ§Ä±klama | SÃ¼re |
|------|------|----------|------|
| **1** | ğŸ¯ BaÄŸlam (Context) | Kelime anlamlarÄ±nÄ±n baÄŸlama gÃ¶re deÄŸiÅŸimi | 20 dk |
| **2** | ğŸ¬ Tek vs Ã‡ift YÃ¶nlÃ¼ | Film metaforu ile unidirectional/bidirectional farkÄ± | 25 dk |
| **3** | ğŸ—ï¸ Transformer Temelleri | Encoder-Decoder mimarisi, Self-Attention | 30 dk |
| **4** | ğŸ¤– GPT vs BERT | Ä°ki mimarinin karÅŸÄ±laÅŸtÄ±rmalÄ± analizi | 20 dk |
| **5** | ğŸ“š MLM & NSP | Masked Language Model ve Next Sentence Prediction | 25 dk |

#### ğŸ’¡ Ã–ne Ã‡Ä±kan Ã–zellikler

- âœ¨ GÃ¼nlÃ¼k hayattan Ã¶rneklerle baÄŸlam kavramÄ±
- ğŸ­ TÃ¼rkÃ§e Ã§ok anlamlÄ± kelimeler (yÃ¼z, kol, anahtar)
- ğŸ“Š Ä°nteraktif Python kod Ã¶rnekleri
- ğŸ’¼ YBS perspektifinden gerÃ§ek dÃ¼nya senaryolarÄ±
- ğŸ¨ GÃ¶rsel diyagramlar ve ASCII art
- ğŸ” AdÄ±m adÄ±m detaylÄ± aÃ§Ä±klamalar

---

### ğŸ““ Notebook 2: WordPiece & Training Fundamentals

**Dosya:** `02.bert-wordpiece-and-training-fundamentals.ipynb`  
**SÃ¼re:** ~180 dakika  
**Seviye:** ğŸŸ¡ Orta â†’ ğŸ”´ Ä°leri

#### ğŸ¯ Ne Ã–ÄŸreneceksiniz?

<div align="left">

```mermaid
%%{init: {'theme':'base', 'themeVariables': { 'primaryColor':'#E5F3FF','primaryTextColor':'#2C3E50','primaryBorderColor':'#99CCFF','lineColor':'#B3D9FF','secondaryColor':'#FFF9E5','tertiaryColor':'#E5FFE5'}}}%%
mindmap
  root((BERT Ä°Ã§ YapÄ±))
    WordPiece
      Tokenization
      Subword Units
      ## Ä°ÅŸareti
    Ã–zel Tokenlar
      CLS Token
      SEP Token
      MASK Token
    Embedding Sistemi
      Token Embedding
      Segment Embedding
      Position Embedding
    Training
      MLM DetaylarÄ±
      NSP DetaylarÄ±
      Fine-tuning
```

</div>

#### ğŸ“‹ BÃ¶lÃ¼m DetaylarÄ±

| AdÄ±m | Konu | AÃ§Ä±klama | SÃ¼re |
|------|------|----------|------|
| **1** | âœ‚ï¸ WordPiece Tokenization | Kelimeleri neden ve nasÄ±l parÃ§alÄ±yoruz | 35 dk |
| **2** | ğŸ·ï¸ Ã–zel Tokenlar | [CLS], [SEP], [MASK] token'larÄ±nÄ±n iÅŸlevi | 30 dk |
| **3** | ğŸ§© ÃœÃ§ KatmanlÄ± Embedding | Token + Segment + Position embedding sistemi | 40 dk |
| **4** | ğŸ“ BERT EÄŸitimi | MLM ve NSP eÄŸitim stratejileri detaylÄ± | 45 dk |
| **5** | ğŸ”„ Fine-tuning | Transfer learning ve downstream gÃ¶revler | 30 dk |

#### ğŸ’¡ Ã–ne Ã‡Ä±kan Ã–zellikler

- ğŸ”¬ Hugging Face Transformers kÃ¼tÃ¼phanesi kullanÄ±mÄ±
- ğŸ› ï¸ GerÃ§ek kod Ã¶rnekleri ve uygulamalar
- ğŸ“Š E-ticaret mÃ¼ÅŸteri yorumu analizi senaryosu
- ğŸ¨ Mermaid diyagramlarÄ± ile gÃ¶rselleÅŸtirme
- ğŸ§ª Ä°nteraktif tokenization deneyleri
- ğŸ’» Pratik fine-tuning Ã¶rnekleri

---

## ğŸ¨ BERT Mimarisi GÃ¶rselleÅŸtirme

### ğŸ—ï¸ BERT Genel Mimari

BERT'in temel mimarisini ve bilgi akÄ±ÅŸÄ±nÄ± gÃ¶steren kapsamlÄ± diyagram:

```mermaid
%%{init: {'theme':'base', 'themeVariables': { 'primaryColor':'#FFF4E6','primaryTextColor':'#2C3E50','primaryBorderColor':'#FFB366','lineColor':'#FF9933','secondaryColor':'#E8F5E9','tertiaryColor':'#E3F2FD','fontSize':'14px'}}}%%
graph TB
    subgraph Input["ğŸ”¤ GÄ°RÄ°Å KATMANI"]
        A["Metin: 'KÃ¶peÄŸim oyun oynamayÄ± seviyor'"]
        B["WordPiece Tokenization"]
        C["[CLS] kÃ¶pek ##im oyun oyna ##ma ##yÄ± sev ##iyor [SEP]"]
    end
    
    subgraph Embed["ğŸ§© EMBEDDING KATMANI"]
        D["Token Embeddings<br/>Kelime temsilleri"]
        E["Segment Embeddings<br/>CÃ¼mle ID'leri"]
        F["Position Embeddings<br/>Pozisyon bilgisi"]
        G["Toplam Embedding<br/>(Token + Segment + Position)"]
    end
    
    subgraph Trans["ğŸ”„ TRANSFORMER ENCODER"]
        H1["Encoder Layer 1<br/>Multi-Head Self-Attention"]
        H2["Encoder Layer 2<br/>Feed Forward"]
        H3["... ... ..."]
        H4["Encoder Layer 12<br/>Contextualized Representations"]
    end
    
    subgraph Output["ğŸ“¤ Ã‡IKTI KATMANI"]
        I["[CLS] Ã‡Ä±ktÄ±<br/>TÃ¼m cÃ¼mle Ã¶zeti"]
        J["Token Ã‡Ä±ktÄ±larÄ±<br/>BaÄŸlamsal vektÃ¶rler"]
    end
    
    subgraph Tasks["ğŸ¯ DOWNSTREAM GÃ–REVLER"]
        K["SÄ±nÄ±flandÄ±rma<br/>Sentiment Analysis"]
        L["NER<br/>Entity Recognition"]
        M["QA<br/>Question Answering"]
    end
    
    A --> B --> C
    C --> D
    C --> E
    C --> F
    D --> G
    E --> G
    F --> G
    G --> H1 --> H2 --> H3 --> H4
    H4 --> I
    H4 --> J
    I --> K
    J --> L
    J --> M
    
    style Input fill:#FFF4E6,stroke:#FFB366,stroke-width:3px,color:#2C3E50
    style Embed fill:#E8F5E9,stroke:#81C784,stroke-width:3px,color:#2C3E50
    style Trans fill:#E3F2FD,stroke:#64B5F6,stroke-width:3px,color:#2C3E50
    style Output fill:#F3E5F5,stroke:#BA68C8,stroke-width:3px,color:#2C3E50
    style Tasks fill:#FFE5E5,stroke:#FF9999,stroke-width:3px,color:#2C3E50
    
    style A fill:#FFE0B2,stroke:#FF9800,stroke-width:2px,color:#2C3E50
    style C fill:#FFF9C4,stroke:#FBC02D,stroke-width:2px,color:#2C3E50
    style G fill:#C8E6C9,stroke:#4CAF50,stroke-width:2px,color:#2C3E50
    style H4 fill:#BBDEFB,stroke:#2196F3,stroke-width:2px,color:#2C3E50
    style I fill:#E1BEE7,stroke:#9C27B0,stroke-width:2px,color:#2C3E50
```

### ğŸ”‘ Mimari BileÅŸenleri

| Katman | Ä°ÅŸlev | Detay |
|--------|-------|-------|
| ğŸ”¤ **GiriÅŸ** | Metin â†’ Token | WordPiece ile parÃ§alama, Ã¶zel token'lar ekleme |
| ğŸ§© **Embedding** | Token â†’ VektÃ¶r | 3 tip embedding'in toplamÄ± (Token+Segment+Position) |
| ğŸ”„ **Transformer** | VektÃ¶r â†’ BaÄŸlam | 12 katman (Base) veya 24 katman (Large) encoder |
| ğŸ“¤ **Ã‡Ä±ktÄ±** | BaÄŸlam â†’ Temsil | Her token iÃ§in baÄŸlamsal vektÃ¶r |
| ğŸ¯ **Task** | Fine-tuning | SÄ±nÄ±flandÄ±rma, NER, QA vb. gÃ¶revler |

---

## ğŸ”„ Tokenization Pipeline

### âœ‚ï¸ WordPiece Tokenization SÃ¼reci

Bir cÃ¼mlenin nasÄ±l token'lara dÃ¶nÃ¼ÅŸtÃ¼rÃ¼ldÃ¼ÄŸÃ¼nÃ¼ adÄ±m adÄ±m gÃ¶steren diyagram:

```mermaid
%%{init: {'theme':'base', 'themeVariables': { 'primaryColor':'#E8F5E9','primaryTextColor':'#1B5E20','primaryBorderColor':'#66BB6A','lineColor':'#4CAF50','secondaryColor':'#FFF3E0','tertiaryColor':'#E1F5FE','fontSize':'13px'}}}%%
graph LR
    subgraph S1["1ï¸âƒ£ HAM METÄ°N"]
        A["ÃœrÃ¼n kaliteli<br/>ve hÄ±zlÄ± geldi"]
    end
    
    subgraph S2["2ï¸âƒ£ NORMALIZE"]
        B["Lowercase<br/>Ã¼rÃ¼n kaliteli<br/>ve hÄ±zlÄ± geldi"]
    end
    
    subgraph S3["3ï¸âƒ£ TOKENIZE"]
        C1["Ã¼rÃ¼n"]
        C2["kalite"]
        C3["##li"]
        C4["ve"]
        C5["hÄ±zlÄ±"]
        C6["gel"]
        C7["##di"]
    end
    
    subgraph S4["4ï¸âƒ£ Ã–ZEL TOKENLAR"]
        D["[CLS] Ã¼rÃ¼n kalite ##li<br/>ve hÄ±zlÄ± gel ##di [SEP]"]
    end
    
    subgraph S5["5ï¸âƒ£ ID'LERE Ã‡EVÄ°R"]
        E["[101, 7854, 2156,<br/>3421, 1005, 8734,<br/>4521, 2134, 102]"]
    end
    
    subgraph S6["6ï¸âƒ£ BERT'E HAZIR"]
        F["Token IDs âœ…<br/>Attention Mask âœ…<br/>Token Type IDs âœ…"]
    end
    
    A --> B
    B --> C1 & C2 & C3 & C4 & C5 & C6 & C7
    C1 --> D
    C2 --> D
    C3 --> D
    C4 --> D
    C5 --> D
    C6 --> D
    C7 --> D
    D --> E
    E --> F
    
    style S1 fill:#FFE5E5,stroke:#FF9999,stroke-width:3px
    style S2 fill:#FFF9E5,stroke:#FFD966,stroke-width:3px
    style S3 fill:#E8F5E9,stroke:#81C784,stroke-width:3px
    style S4 fill:#E3F2FD,stroke:#64B5F6,stroke-width:3px
    style S5 fill:#F3E5F5,stroke:#BA68C8,stroke-width:3px
    style S6 fill:#C8E6C9,stroke:#4CAF50,stroke-width:4px
    
    style A fill:#FFCDD2,stroke:#E57373,color:#2C3E50
    style B fill:#FFF9C4,stroke:#FFF176,color:#2C3E50
    style D fill:#BBDEFB,stroke:#64B5F6,color:#2C3E50
    style E fill:#E1BEE7,stroke:#BA68C8,color:#2C3E50
    style F fill:#A5D6A7,stroke:#66BB6A,stroke-width:3px,color:#1B5E20
```

### ğŸ” Tokenization Ã–rnek Analizi

| Orijinal | Token'lar | AÃ§Ä±klama |
|----------|-----------|----------|
| **"playing"** | `["play", "##ing"]` | KÃ¶k + ek ayrÄ±mÄ± |
| **"unbelievable"** | `["un", "##believe", "##able"]` | Ã–nek + kÃ¶k + sonek |
| **"kitaplÄ±k"** | `["kitap", "##lÄ±k"]` | TÃ¼rkÃ§e kÃ¶k + ek |
| **"oynamayÄ±"** | `["oyun", "##ma", "##yÄ±"]` | Ã‡oklu ek yapÄ±sÄ± |

> ğŸ’¡ **Not:** `##` iÅŸareti, token'Ä±n kelimenin baÅŸÄ±nda deÄŸil, devamÄ± olduÄŸunu gÃ¶sterir!

---

## ğŸ“ Training Pipeline

### ğŸ“š BERT EÄŸitim SÃ¼reci

BERT'in nasÄ±l eÄŸitildiÄŸini gÃ¶steren tam pipeline:

```mermaid
%%{init: {'theme':'base', 'themeVariables': { 'primaryColor':'#E3F2FD','primaryTextColor':'#0D47A1','primaryBorderColor':'#42A5F5','lineColor':'#2196F3','secondaryColor':'#FFF3E0','tertiaryColor':'#F3E5F5','fontSize':'13px'}}}%%
graph TB
    subgraph Data["ğŸ“Š VERÄ° HAZIRLIÄI"]
        A["BÃ¼yÃ¼k Metin Korpusu<br/>Wikipedia + Books<br/>3.3 Milyar Kelime"]
        B["CÃ¼mle Ã‡iftleri OluÅŸtur<br/>(Sentence A, Sentence B)"]
        C["50% ArdÄ±ÅŸÄ±k<br/>50% Rastgele"]
    end
    
    subgraph MLM["ğŸ­ MASKED LANGUAGE MODEL"]
        D["Her cÃ¼mlenin %15'ini<br/>rastgele maskele"]
        E["80% [MASK]<br/>10% Rastgele<br/>10% Orijinal"]
        F["Model [MASK]'Ä±<br/>tahmin etmeye Ã§alÄ±ÅŸÄ±r"]
    end
    
    subgraph NSP["ğŸ”— NEXT SENTENCE PREDICTION"]
        G["CÃ¼mle A ve B<br/>veriliyor"]
        H["B, A'nÄ±n devamÄ± mÄ±?<br/>Ä°kili sÄ±nÄ±flandÄ±rma"]
        I["IsNext: 1<br/>NotNext: 0"]
    end
    
    subgraph Train["âš™ï¸ EÄÄ°TÄ°M"]
        J["MLM Loss +<br/>NSP Loss"]
        K["Backpropagation<br/>Weight Update"]
        L["64 TPU Ã‡ipi<br/>4 GÃ¼n EÄŸitim"]
    end
    
    subgraph PreTrain["âœ… PRE-TRAINED MODEL"]
        M["BERT Base<br/>110M Parameters"]
        N["Genel Dil AnlayÄ±ÅŸÄ±<br/>KazandÄ±"]
    end
    
    subgraph FineTune["ğŸ”§ FINE-TUNING"]
        O["Task-Specific<br/>Dataset"]
        P["SÄ±nÄ±flandÄ±rma KatmanÄ±<br/>Ekle"]
        Q["Az Veriyle<br/>HÄ±zlÄ± EÄŸitim"]
        R["Spesifik GÃ¶rev<br/>Ä°Ã§in HazÄ±r"]
    end
    
    A --> B --> C
    C --> D
    D --> E --> F
    C --> G
    G --> H --> I
    F --> J
    I --> J
    J --> K --> L
    L --> M --> N
    N --> O --> P --> Q --> R
    
    style Data fill:#FFF4E6,stroke:#FFB366,stroke-width:3px,color:#2C3E50
    style MLM fill:#FFE5E5,stroke:#FF9999,stroke-width:3px,color:#2C3E50
    style NSP fill:#E8F5E9,stroke:#81C784,stroke-width:3px,color:#2C3E50
    style Train fill:#E3F2FD,stroke:#64B5F6,stroke-width:3px,color:#0D47A1
    style PreTrain fill:#F3E5F5,stroke:#BA68C8,stroke-width:4px,color:#2C3E50
    style FineTune fill:#FFF9E5,stroke:#FFD966,stroke-width:3px,color:#2C3E50
    
    style M fill:#E1BEE7,stroke:#9C27B0,stroke-width:3px,color:#2C3E50
    style R fill:#C8E6C9,stroke:#4CAF50,stroke-width:3px,color:#1B5E20
```

### ğŸ¯ EÄŸitim Stratejileri KarÅŸÄ±laÅŸtÄ±rma

<table>
<tr>
<th width="25%">ğŸ­ MLM</th>
<th width="25%">ğŸ”— NSP</th>
<th width="25%">âš–ï¸ Combined</th>
<th width="25%">ğŸ”§ Fine-tuning</th>
</tr>
<tr>
<td>

**AmaÃ§:**  
MaskelenmiÅŸ kelimeleri tahmin et

**Ã–rnek:**  
"KÃ¶peÄŸim [MASK] seviyor"  
â†’ "oynamayÄ±"

</td>
<td>

**AmaÃ§:**  
Ä°ki cÃ¼mle ardÄ±ÅŸÄ±k mÄ±?

**Ã–rnek:**  
A: "Hava gÃ¼zel"  
B: "YÃ¼rÃ¼yÃ¼ÅŸe Ã§Ä±ktÄ±m"  
â†’ IsNext âœ…

</td>
<td>

**SonuÃ§:**  
Ä°ki gÃ¶rev birlikte

**Fayda:**  
Hem kelime hem de cÃ¼mle seviyesi anlama

</td>
<td>

**SÃ¼reÃ§:**  
KÃ¼Ã§Ã¼k dataset ile hÄ±zlÄ± adaptasyon

**Avantaj:**  
Transfer learning gÃ¼cÃ¼

</td>
</tr>
</table>

---

## ğŸš€ NasÄ±l KullanÄ±lÄ±r

### ğŸ“¥ 1. Gerekli Kurulumlar

```bash
# Python ortamÄ± oluÅŸtur (isteÄŸe baÄŸlÄ± ama Ã¶nerilen)
python -m venv bert_env
bert_env\Scripts\activate  # Windows
# source bert_env/bin/activate  # Linux/Mac

# Gerekli kÃ¼tÃ¼phaneleri yÃ¼kle
pip install transformers torch jupyter numpy pandas matplotlib
```

### ğŸ“š 2. Notebook'larÄ± Ã‡alÄ±ÅŸtÄ±rma

```bash
# Jupyter Notebook baÅŸlat
jupyter notebook

# TarayÄ±cÄ±da aÃ§Ä±lan sayfadan sÄ±rasÄ±yla:
# 1. 01.bertpaper-terminology.ipynb
# 2. 02.bert-wordpiece-and-training-fundamentals.ipynb
```

### ğŸ¯ 3. Ã–nerilen Ã‡alÄ±ÅŸma SÄ±rasÄ±

```mermaid
%%{init: {'theme':'base', 'themeVariables': { 'primaryColor':'#E8F5E9','primaryTextColor':'#1B5E20','primaryBorderColor':'#66BB6A','lineColor':'#4CAF50'}}}%%
graph LR
    A["ğŸ““ Notebook 1<br/>Terminoloji"] --> B["ğŸ““ Notebook 2<br/>Ä°Ã§ YapÄ±"]
    B --> C["ğŸ’» Pratik<br/>Projeler"]
    C --> D["ğŸ”§ Fine-tuning<br/>Denemeleri"]
    
    style A fill:#FFE5E5,stroke:#FF9999,stroke-width:3px,color:#2C3E50
    style B fill:#E3F2FD,stroke:#64B5F6,stroke-width:3px,color:#2C3E50
    style C fill:#FFF9E5,stroke:#FFD966,stroke-width:3px,color:#2C3E50
    style D fill:#C8E6C9,stroke:#4CAF50,stroke-width:3px,color:#1B5E20
```

---

## ğŸ“š Ã–ÄŸrenme Yol HaritasÄ±

### ğŸ—“ï¸ HaftalÄ±k Plan

<table>
<tr>
<th width="15%">Hafta</th>
<th width="35%">Konu</th>
<th width="30%">Aktivite</th>
<th width="20%">Hedef</th>
</tr>
<tr>
<td align="center"><strong>1ï¸âƒ£</strong></td>
<td>ğŸ““ <strong>Notebook 1</strong><br/>BERT Terminoloji</td>
<td>
â€¢ Her bÃ¶lÃ¼mÃ¼ dikkatlice oku<br/>
â€¢ Kod Ã¶rneklerini Ã§alÄ±ÅŸtÄ±r<br/>
â€¢ Flash card'larÄ± ezberle
</td>
<td>Temel kavramlarÄ± anlama</td>
</tr>
<tr>
<td align="center"><strong>2ï¸âƒ£</strong></td>
<td>ğŸ““ <strong>Notebook 2</strong><br/>WordPiece & Training</td>
<td>
â€¢ Tokenization deneyleri yap<br/>
â€¢ Embedding sistemini Ã¶ÄŸren<br/>
â€¢ MLM/NSP'yi uygula
</td>
<td>Ä°Ã§ mekanikleri kavrama</td>
</tr>
<tr>
<td align="center"><strong>3ï¸âƒ£</strong></td>
<td>ğŸ’» <strong>Pratik Proje</strong><br/>Kendi Dataset'in</td>
<td>
â€¢ Hugging Face kullan<br/>
â€¢ Sentiment analysis yap<br/>
â€¢ Kendi verinle fine-tune et
</td>
<td>GerÃ§ek uygulama deneyimi</td>
</tr>
<tr>
<td align="center"><strong>4ï¸âƒ£</strong></td>
<td>ğŸš€ <strong>Ä°leri Seviye</strong><br/>Optimizasyon</td>
<td>
â€¢ FarklÄ± BERT varyantlarÄ±<br/>
â€¢ Hiperparametre tuning<br/>
â€¢ Production deployment
</td>
<td>Profesyonel seviye</td>
</tr>
</table>

### ğŸ“ Ã–ÄŸrenme Ä°puÃ§larÄ±

> ğŸ’¡ **BaÅŸarÄ± iÃ§in 5 AltÄ±n Kural:**
>
> 1. **ğŸ“– SabÄ±rlÄ± Ol:** BERT karmaÅŸÄ±k bir mimari, her ÅŸeyi ilk seferde anlamak zorunda deÄŸilsin
> 2. **ğŸ’» Pratik Yap:** Sadece okumakla yetinme, her kod bloÄŸunu Ã§alÄ±ÅŸtÄ±r
> 3. **ğŸ¤” Soru Sor:** AnlamadÄ±ÄŸÄ±n yerleri iÅŸaretle ve araÅŸtÄ±r
> 4. **ğŸ”„ Tekrar Et:** BazÄ± kavramlarÄ± anlamak iÃ§in 2-3 kez gÃ¶zden geÃ§ir
> 5. **ğŸ‘¥ PaylaÅŸ:** Ã–ÄŸrendiklerini baÅŸkalarÄ±na anlat, bu en iyi pekiÅŸtirme yÃ¶ntemidir

---

## âš™ï¸ Gereksinimler

### ğŸ Python ve KÃ¼tÃ¼phaneler

```python
# requirements.txt
transformers>=4.35.0
torch>=2.0.0
jupyter>=1.0.0
numpy>=1.24.0
pandas>=2.0.0
matplotlib>=3.7.0
seaborn>=0.12.0
```

### ğŸ’» Sistem Gereksinimleri

| BileÅŸen | Minimum | Ã–nerilen |
|---------|---------|----------|
| **Python** | 3.8+ | 3.10+ |
| **RAM** | 8 GB | 16 GB+ |
| **Disk** | 5 GB | 10 GB+ |
| **GPU** | Yok (CPU ile Ã§alÄ±ÅŸÄ±r) | NVIDIA GPU (CUDA) |

### ğŸŒ Online Alternatifler

Yerel kurulum yapamÄ±yorsan:
- ğŸ”µ **Google Colab:** Ãœcretsiz GPU ile Ã§alÄ±ÅŸtÄ±r
- ğŸŸ£ **Kaggle Notebooks:** HazÄ±r ortamda dene
- ğŸŸ¢ **Hugging Face Spaces:** Demo oluÅŸtur ve paylaÅŸ

---

## ğŸ’¡ Ä°puÃ§larÄ±

### ğŸ¯ Verimli Ã‡alÄ±ÅŸma Stratejileri

<table>
<tr>
<td width="50%">

**âœ… YAPILMASI GEREKENLER**

- ğŸ“ Her bÃ¶lÃ¼m sonunda notlar al
- ğŸ’» Kod Ã¶rneklerini deÄŸiÅŸtirerek dene
- ğŸ¨ DiyagramlarÄ± kendi kelimelerinle Ã§iz
- ğŸ¤” "Neden?" sorularÄ±nÄ± sor
- ğŸ“Š FarklÄ± Ã¶rneklerle test et
- ğŸ‘¥ Ã‡alÄ±ÅŸma grubu oluÅŸtur

</td>
<td width="50%">

**âŒ YAPILMAMASI GEREKENLER**

- ğŸš« Sadece hÄ±zlÄ±ca gÃ¶z gezdirme
- ğŸš« Kod Ã§alÄ±ÅŸtÄ±rmadan geÃ§me
- ğŸš« AnlamamÄ±ÅŸken devam etme
- ğŸš« Bir seferde hepsini bitirmeye Ã§alÄ±ÅŸma
- ğŸš« Pratik yapmadan teoride kalma
- ğŸš« YalnÄ±z Ã§alÄ±ÅŸÄ±p yardÄ±m istememe

</td>
</tr>
</table>

### ğŸ› SÄ±k KarÅŸÄ±laÅŸÄ±lan Sorunlar

<details>
<summary><strong>â“ BERT Ã§ok yavaÅŸ Ã§alÄ±ÅŸÄ±yor</strong></summary>

**Ã‡Ã¶zÃ¼mler:**
- Daha kÃ¼Ã§Ã¼k bir model kullan: `bert-base-uncased` â†’ `distilbert-base-uncased`
- Batch size'Ä± azalt
- Sequence length'i kÄ±salt (512 â†’ 128)
- GPU kullan veya Google Colab'a geÃ§

</details>

<details>
<summary><strong>â“ "Out of Memory" hatasÄ± alÄ±yorum</strong></summary>

**Ã‡Ã¶zÃ¼mler:**
- Batch size'Ä± kÃ¼Ã§Ã¼lt (32 â†’ 16 â†’ 8)
- Gradient accumulation kullan
- Mixed precision training dene (`fp16=True`)
- Daha kÃ¼Ã§Ã¼k bir model seÃ§

</details>

<details>
<summary><strong>â“ Tokenizer TÃ¼rkÃ§e metinlerde iyi Ã§alÄ±ÅŸmÄ±yor</strong></summary>

**Ã‡Ã¶zÃ¼mler:**
- TÃ¼rkÃ§e iÃ§in Ã¶zel eÄŸitilmiÅŸ model kullan: `dbmdz/bert-base-turkish-cased`
- Veya multilingual model: `bert-base-multilingual-cased`
- Kendi tokenizer'Ä±nÄ±zÄ± eÄŸitin (ileri seviye)

</details>

<details>
<summary><strong>â“ Fine-tuning sonuÃ§larÄ± kÃ¶tÃ¼</strong></summary>

**Ã‡Ã¶zÃ¼mler:**
- Learning rate'i ayarla (genellikle 2e-5 - 5e-5 arasÄ±)
- Daha fazla epoch dene
- Dataset'ini dengele (class imbalance kontrolÃ¼)
- Pre-trained model seÃ§imini gÃ¶zden geÃ§ir
- Data augmentation uygula

</details>

---

## ğŸ“– Kaynaklar

### ğŸ“„ Orijinal Makaleler

1. **BERT Paper (2018)**  
   [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)  
   *Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova*

2. **Attention is All You Need (2017)**  
   [Transformer Architecture](https://arxiv.org/abs/1706.03762)  
   *Vaswani et al.*

### ğŸŒ Online Kaynaklar

| Kaynak | AÃ§Ä±klama | Link |
|--------|----------|------|
| ğŸ¤— **Hugging Face** | BERT DÃ¶kÃ¼mantasyonu | [transformers.docs](https://huggingface.co/docs/transformers) |
| ğŸ“º **Jay Alammar** | GÃ¶rsel BERT AÃ§Ä±klamalarÄ± | [The Illustrated BERT](http://jalammar.github.io/illustrated-bert/) |
| ğŸ“š **Papers With Code** | BERT Implementations | [paperswithcode.com](https://paperswithcode.com/method/bert) |
| ğŸ“ **Stanford CS224N** | NLP Dersleri | [web.stanford.edu/class/cs224n/](https://web.stanford.edu/class/cs224n/) |

### ğŸ“š TÃ¼rkÃ§e Kaynaklar

- ğŸ‡¹ğŸ‡· **TÃ¼rkÃ§e NLP Workshop:** Pratik Ã¶rnekler ve topluluk
- ğŸ‡¹ğŸ‡· **Turkish BERT Models:** `dbmdz/bert-base-turkish-cased`
- ğŸ‡¹ğŸ‡· **NLP Turkey Community:** Discord ve GitHub grubu

### ğŸ¥ Video KaynaklarÄ±

- ğŸ“º **StatQuest:** BERT clearly explained (Ä°ngilizce, gÃ¶rsel)
- ğŸ“º **Yannic Kilcher:** BERT paper walkthrough (DetaylÄ±)
- ğŸ“º **DeepLearning.AI:** NLP Specialization (Coursera)

---

## â­ TeÅŸekkÃ¼rler

Bu notebook'larÄ± faydalÄ± bulduysan:
- â­ GitHub repo'ya yÄ±ldÄ±z ver
- ğŸ”„ ArkadaÅŸlarÄ±nla paylaÅŸ
- ğŸ“ Geri bildirim bÄ±rak
- ğŸ¤ TopluluÄŸa katÄ±l

---

<div align="center">

### ğŸ“ Mutlu Ã–ÄŸrenmeler! 

**"BERT'Ã¼ anlamak, modern NLP'nin kapÄ±larÄ±nÄ± aÃ§maktÄ±r."**

Made with â¤ï¸ for NLP Enthusiasts by Cemal YÃœKSEL | 2026

---

â¬†ï¸ [BaÅŸa DÃ¶n](#-iÌ‡Ã§indekiler)

</div>
